####List the files in cloudxlab
[nandagnk2141@cxln5 datasets]$ pwd
/home/nandagnk2141/nanda/pyspark/datasets
[nandagnk2141@cxln5 datasets]$ ls -ltr
total 77256
-rw-rw-r-- 1 nandagnk2141 nandagnk2141   82800 May 12  2017 usdata.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141    2042 Jul  8  2022 JOIN DATA SETS NEW.txt
-rw-rw-r-- 1 nandagnk2141 nandagnk2141   20352 Jul  9  2022 WebData_Day1.txt
-rw-rw-r-- 1 nandagnk2141 nandagnk2141    3307 Jul  9  2022 WebData_Day2.txt
-rw-rw-r-- 1 nandagnk2141 nandagnk2141      24 Aug 25  2022 Sales.txt
-rw-rw-r-- 1 nandagnk2141 nandagnk2141    3595 Aug 25  2022 noc_locations.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 1025109 Aug 25  2022 customer_records.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 8208121 Aug 25  2022 txs.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 5816923 Aug 25  2022 weatherHistory.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 4485608 Sep  7  2022 txs_UPD.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 5816841 Sep  9  2022 weatherHistory_Noheader.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141     981 Sep 16  2022 actors.json
-rw-rw-r-- 1 nandagnk2141 nandagnk2141     510 Sep 16  2022 account_sales.json
-rw-rw-r-- 1 nandagnk2141 nandagnk2141    1407 Sep 16  2022 array_complex_json.json
-rw-rw-r-- 1 nandagnk2141 nandagnk2141  155668 Sep 16  2022 Adeline_bank_json.json
-rw-rw-r-- 1 nandagnk2141 nandagnk2141    6601 Sep 16  2022 Array_dat.json
-rw-rw-r-- 1 nandagnk2141 nandagnk2141   57679 Sep 16  2022 devices.json
-rw-rw-r-- 1 nandagnk2141 nandagnk2141     705 Sep 16  2022 jsn_str.json
-rw-rw-r-- 1 nandagnk2141 nandagnk2141     171 Sep 16  2022 json_strct.json
-rw-rw-r-- 1 nandagnk2141 nandagnk2141     207 Sep 16  2022 place.json
-rw-rw-r-- 1 nandagnk2141 nandagnk2141     108 Sep 17  2022 comp_data.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141     241 Feb 10  2023 Item_Sales.txt
-rw-rw-r-- 1 nandagnk2141 nandagnk2141     131 Feb 24 15:00 CourseDetails.txt
-rw-rw-r-- 1 nandagnk2141 nandagnk2141    4431 Mar  4 20:19 book.xml
-rw-rw-r-- 1 nandagnk2141 nandagnk2141      41 Mar 12 22:59 dept_data.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 8208206 Apr 26 10:45 transactions.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141     170 Aug 12 15:25 Nanda_Sample_1.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141      45 Aug 14 09:03 bank.txt
-rw-rw-r-- 1 nandagnk2141 nandagnk2141      90 Aug 14 09:03 employee.txt
-rw-rw-r-- 1 nandagnk2141 nandagnk2141     679 Aug 16 10:37 customer_records_UPD.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 8149504 Aug 18 10:14 transactions.xls
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 8208121 Aug 18 10:58 transactions_upd.txt
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 8208121 Aug 18 11:48 transactions_upd.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141     153 Aug 20 06:04 emp_data.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141      27 Aug 20 06:41 IPL.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 6799807 Aug 20 14:05 athlete_events.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 5606519 Aug 20 14:17 transactions_upd_1.csv
-rw-rw-r-- 1 nandagnk2141 nandagnk2141 8085792 Aug 22 08:46 txn.csv.filepart

#####Connect to cloudxlab terminal and start pyspark
[nandagnk2141@cxln5 datasets]$ pyspark
SPARK_MAJOR_VERSION is set to 2, using Spark2
Python 2.7.5 (default, Apr  9 2019, 14:30:50) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/08/22 08:47:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/08/22 08:47:02 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/08/22 08:47:02 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/08/22 08:47:02 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.1.1.2.6.2.0-205
      /_/
Using Python version 2.7.5 (default, Apr  9 2019 14:30:50)
SparkSession available as 'spark'.
>>> 

=================================================================================================================
sample code to load csv file
spark.read.format("csv")\
                .options(inferschema=True,delimiter=",")\
                .load("file:////home/nandagnk2141/nanda/pyspark/datasets/txs.csv")\
                .toDF("txn_id","txn_date","cust_id","amount","category","product","city","state","payment_type")
=================================================================================================================				

>>> df = spark.read.format("csv")\
...                 .options(inferschema=True,delimiter=",")\
...                 .load("file:////home/nandagnk2141/nanda/pyspark/datasets/txs.csv")\
...                 .toDF("txn_id","txn_date","cust_id","amount","category","product","city","state","payment_type")
>>> df.printSchema()
root
 |-- txn_id: integer (nullable = true)
 |-- txn_date: string (nullable = true)
 |-- cust_id: integer (nullable = true)
 |-- amount: double (nullable = true)
 |-- category: string (nullable = true)
 |-- product: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- payment_type: string (nullable = true)
				
>>> df.show(5,truncate=False)
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+
|txn_id|txn_date  |cust_id|amount|category          |product                          |city       |state     |payment_type|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+
|0     |06-26-2011|4007024|40.33 |null              |Cardio Machine Accessories       |Clarksville|Tennessee |credit      |
|1     |05-26-2011|4006742|198.44|Exercise & Fitness|Weightlifting Gloves             |Long Beach |California|credit      |
|2     |06-01-2011|4009775|5.58  |Exercise & Fitness|Weightlifting Machine Accessories|Anaheim    |California|credit      |
|3     |06-05-2011|4002199|198.19|Gymnastics        |Gymnastics Rings                 |Milwaukee  |Wisconsin |credit      |
|4     |12-17-2011|4002613|98.81 |Team Sports       |Field Hockey                     |Nashville  |Tennessee |credit      |
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+
only showing top 5 rows

>>> df.rdd.getNumPartitions()
2
>>> df.groupby("product").sum("amount").show()
+-----------------+-----------------+
|          product|      sum(amount)|
+-----------------+-----------------+
|     Field Hockey|80791.25999999998|
|     Ice Climbing|79212.28999999998|
|           Tennis|81971.09000000001|
|           Boxing|85508.58999999997|
|       Playhouses|          80646.1|
|          Hunting|         76229.92|
|        Ping Pong|77114.57000000004|
|             Golf|85222.72999999998|
|     Water Tables|78235.82999999999|
|     Water Tubing|85526.42999999996|
|         Softball|79113.36000000004|
|  Gymnastics Mats|         83076.69|
|   Jigsaw Puzzles|80967.13999999998|
|         Swimming|83559.98999999995|
|         Sledding|76687.65000000002|
|Lawn Water Slides|82020.85000000003|
|   Exercise Balls|88820.39000000001|
|  Riding Scooters|77861.43999999993|
|       Jump Ropes|82464.30999999998|
|       Bingo Sets|78390.48999999999|
+-----------------+-----------------+
only showing top 20 rows

>>> df.rdd.getNumPartitions()
2

now creating a new dataframe from df with a groupby clause
>>> df2 = df.groupby("product").sum("amount")
>>> df2.show(10,truncate=False)
+------------+-----------------+
|product     |sum(amount)      |
+------------+-----------------+
|Field Hockey|80791.25999999998|
|Ice Climbing|79212.28999999998|
|Tennis      |81971.09000000001|
|Boxing      |85508.58999999997|
|Playhouses  |80646.1          |
|Hunting     |76229.92         |
|Ping Pong   |77114.57000000004|
|Golf        |85222.72999999998|
|Water Tables|78235.82999999999|
|Water Tubing|85526.42999999996|
+------------+-----------------+
only showing top 10 rows

>>> df2.rdd.getNumPartitions()
200

>>> spark.conf.get("spark.sql.shuffle.partitions")
u'200'

to set the value spark.conf.set("spark.sql.shuffle.partitions",Value) and this is session specific, will reset once the session is closed.

>>> spark.conf.set("spark.sql.shuffle.partitions",50)
>>> spark.conf.get("spark.sql.shuffle.partitions")
u'50'
>>> 

to test this create a new dataframe with a group by clause and check 
df3 = df.groupby("state","city").max("amount")

>>> df3 = df.groupby("state","city").max("amount")
>>> df3.rdd.getNumPartitions()
50
>>> 

now close this session and open a new session and try the above it should set to 200 again.

>>> exit()
[nandagnk2141@cxln5 datasets]$ pyspark
SPARK_MAJOR_VERSION is set to 2, using Spark2
Python 2.7.5 (default, Apr  9 2019, 14:30:50) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/08/22 09:09:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/08/22 09:09:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/08/22 09:09:49 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/08/22 09:09:49 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.1.1.2.6.2.0-205
      /_/
Using Python version 2.7.5 (default, Apr  9 2019 14:30:50)
SparkSession available as 'spark'.
>>> spark.read.format("csv")\
...                 .options(inferschema=True,delimiter=",")\
...                 .load("file:////home/nandagnk2141/nanda/pyspark/datasets/txs.csv")\
...                 .toDF("txn_id","txn_date","cust_id","amount","category","product","city","state","payment_type")
DataFrame[txn_id: int, txn_date: string, cust_id: int, amount: double, category: string, product: string, city: string, state: string, payment_type: string]
>>> df = spark.read.format("csv")\
...                 .options(inferschema=True,delimiter=",")\
...                 .load("file:////home/nandagnk2141/nanda/pyspark/datasets/txs.csv")\
...                 .toDF("txn_id","txn_date","cust_id","amount","category","product","city","state","payment_type")
>>> df2=df.groupby("product").sum("amount")
>>> df2.rdd.getNumPartitions()
200

================================================================================================================
How to handle date column in pyspark?
=====================================

1. add date and timestamp

from pyspark.sql import SparkSession
from pyspark.sql.functions import sum,col,round,max,min,avg,count,desc,expr,concat,lit,when
from pyspark.sql.functions import *

if __name__ == '__main__':
    print('Application Batch14 PySpark SPARK-SQL Assignment 1')
    spark = SparkSession.builder.appName("Working with DSL").master("local").getOrCreate()
    sc = spark.sparkContext
    spark.sparkContext.setLogLevel("Error")

    df_txn = spark.read.format("csv")\
                .options(inferschema=True,delimiter=",")\
                .load("file:///D:/Practising/PySpark/DATASETS/txs.csv")\
                .toDF("txn_id","txn_date","cust_id","amount","category","product","city","state","payment_type")

    df_txn.printSchema()
    #df_txn.show(5,truncate=False)

    #scenario 1 add a new column load_date and show current date.
    df_txn.withColumn("load_date",current_date()).show(5,truncate=False)

    # scenario 2 add a new column load_time and show current time.
    df_txn.withColumn("load_date", current_timestamp()).show(5, truncate=False)

    
output
======

root
 |-- txn_id: integer (nullable = true)
 |-- txn_date: string (nullable = true)
 |-- cust_id: integer (nullable = true)
 |-- amount: double (nullable = true)
 |-- category: string (nullable = true)
 |-- product: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- payment_type: string (nullable = true)

+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+
|txn_id|txn_date  |cust_id|amount|category          |product                          |city       |state     |payment_type|load_date |
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+
|0     |06-26-2011|4007024|40.33 |null              |Cardio Machine Accessories       |Clarksville|Tennessee |credit      |2023-08-22|
|1     |05-26-2011|4006742|198.44|Exercise & Fitness|Weightlifting Gloves             |Long Beach |California|credit      |2023-08-22|
|2     |06-01-2011|4009775|5.58  |Exercise & Fitness|Weightlifting Machine Accessories|Anaheim    |California|credit      |2023-08-22|
|3     |06-05-2011|4002199|198.19|Gymnastics        |Gymnastics Rings                 |Milwaukee  |Wisconsin |credit      |2023-08-22|
|4     |12-17-2011|4002613|98.81 |Team Sports       |Field Hockey                     |Nashville  |Tennessee |credit      |2023-08-22|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+
only showing top 5 rows

+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+-----------------------+
|txn_id|txn_date  |cust_id|amount|category          |product                          |city       |state     |payment_type|load_time              |
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+-----------------------+
|0     |06-26-2011|4007024|40.33 |null              |Cardio Machine Accessories       |Clarksville|Tennessee |credit      |2023-08-22 22:03:32.292|
|1     |05-26-2011|4006742|198.44|Exercise & Fitness|Weightlifting Gloves             |Long Beach |California|credit      |2023-08-22 22:03:32.292|
|2     |06-01-2011|4009775|5.58  |Exercise & Fitness|Weightlifting Machine Accessories|Anaheim    |California|credit      |2023-08-22 22:03:32.292|
|3     |06-05-2011|4002199|198.19|Gymnastics        |Gymnastics Rings                 |Milwaukee  |Wisconsin |credit      |2023-08-22 22:03:32.292|
|4     |12-17-2011|4002613|98.81 |Team Sports       |Field Hockey                     |Nashville  |Tennessee |credit      |2023-08-22 22:03:32.292|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+-----------------------+
only showing top 5 rows


Process finished with exit code 0


=================================================================================
    # scenario 3 add a new column unix_time and show unix timestamp
    # unix timestamp will show the no of seconds from 1 Jan 1970
    # reason for using this one is deployment is going to happen on Unix/linux so,
    # it handles the unix timestamp easy
    df_txn.withColumn("unix_timestamp", unix_timestamp()).show(5, truncate=False)
	
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+--------------+
|txn_id|txn_date  |cust_id|amount|category          |product                          |city       |state     |payment_type|unix_timestamp|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+--------------+
|0     |06-26-2011|4007024|40.33 |null              |Cardio Machine Accessories       |Clarksville|Tennessee |credit      |1692706162    |
|1     |05-26-2011|4006742|198.44|Exercise & Fitness|Weightlifting Gloves             |Long Beach |California|credit      |1692706162    |
|2     |06-01-2011|4009775|5.58  |Exercise & Fitness|Weightlifting Machine Accessories|Anaheim    |California|credit      |1692706162    |
|3     |06-05-2011|4002199|198.19|Gymnastics        |Gymnastics Rings                 |Milwaukee  |Wisconsin |credit      |1692706162    |
|4     |12-17-2011|4002613|98.81 |Team Sports       |Field Hockey                     |Nashville  |Tennessee |credit      |1692706162    |
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+--------------+
only showing top 5 rows


datatype of unix timestamp will be long.


root
 |-- txn_id: integer (nullable = true)
 |-- txn_date: string (nullable = true)
 |-- cust_id: integer (nullable = true)
 |-- amount: double (nullable = true)
 |-- category: string (nullable = true)
 |-- product: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- payment_type: string (nullable = true)
 |-- unix_timestamp: long (nullable = true)
	
	
============================================================================================================

# scenario 4 convert string to date
# convert the txn_date from string to date	

df_txn.withColumn("txn_date",to_date(df_txn["txn_date"],"mm-dd-yyyy")).show(5,truncate=False)

+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+
|txn_id|txn_date  |cust_id|amount|category          |product                          |city       |state     |payment_type|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+
|0     |2011-01-26|4007024|40.33 |null              |Cardio Machine Accessories       |Clarksville|Tennessee |credit      |
|1     |2011-01-26|4006742|198.44|Exercise & Fitness|Weightlifting Gloves             |Long Beach |California|credit      |
|2     |2011-01-01|4009775|5.58  |Exercise & Fitness|Weightlifting Machine Accessories|Anaheim    |California|credit      |
|3     |2011-01-05|4002199|198.19|Gymnastics        |Gymnastics Rings                 |Milwaukee  |Wisconsin |credit      |
|4     |2011-01-17|4002613|98.81 |Team Sports       |Field Hockey                     |Nashville  |Tennessee |credit      |
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+
only showing top 5 rows

Note: txn_date was given in the format "mm-dd-yyyy" in the input file, hence passing the same format while converting to date column.

df_txn_date = df_txn.withColumn("txn_date",to_date(df_txn["txn_date"],"mm-dd-yyyy"))
df_txn_date.printSchema()

root
 |-- txn_id: integer (nullable = true)
 |-- txn_date: date (nullable = true)
 |-- cust_id: integer (nullable = true)
 |-- amount: double (nullable = true)
 |-- category: string (nullable = true)
 |-- product: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- payment_type: string (nullable = true)
 
 Note: earlier txn_date was string datatype, now changed as date.
 
 
 similary txn_date can be converted from string to timestamp datatype.
 
 # scenario 5 convert string to timestamp
 # convert the txn_date from string to timestamp

df_txn.withColumn("txn_date",to_timestamp(df_txn["txn_date"],"mm-dd-yyyy")).show(5,truncate=False)
+------+-------------------+-------+------+------------------+---------------------------------+-----------+----------+------------+
|txn_id|txn_date           |cust_id|amount|category          |product                          |city       |state     |payment_type|
+------+-------------------+-------+------+------------------+---------------------------------+-----------+----------+------------+
|0     |2011-01-26 00:06:00|4007024|40.33 |null              |Cardio Machine Accessories       |Clarksville|Tennessee |credit      |
|1     |2011-01-26 00:05:00|4006742|198.44|Exercise & Fitness|Weightlifting Gloves             |Long Beach |California|credit      |
|2     |2011-01-01 00:06:00|4009775|5.58  |Exercise & Fitness|Weightlifting Machine Accessories|Anaheim    |California|credit      |
|3     |2011-01-05 00:06:00|4002199|198.19|Gymnastics        |Gymnastics Rings                 |Milwaukee  |Wisconsin |credit      |
|4     |2011-01-17 00:12:00|4002613|98.81 |Team Sports       |Field Hockey                     |Nashville  |Tennessee |credit      |
+------+-------------------+-------+------+------------------+---------------------------------+-----------+----------+------------+

df_txn_date = df_txn.withColumn("txn_date",to_timestamp(df_txn["txn_date"],"mm-dd-yyyy"))
df_txn_date.printSchema()

root
 |-- txn_id: integer (nullable = true)
 |-- txn_date: timestamp (nullable = true)
 |-- cust_id: integer (nullable = true)
 |-- amount: double (nullable = true)
 |-- category: string (nullable = true)
 |-- product: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- payment_type: string (nullable = true)

============================================================================================================

# scenario 6 convert string column to date column and change the date format DD/MM/YYYY while displaying the date
# convert the txn_date from string to date and change the date format DD/MM/YYYY while displaying the date

df_txn.withColumn("txn_date", date_format(to_timestamp(df_txn["txn_date"], "mm-dd-yyyy"),"dd/mm/yyyy")).show(5, truncate=False)

+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+
|txn_id|txn_date  |cust_id|amount|category          |product                          |city       |state     |payment_type|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+
|0     |26/06/2011|4007024|40.33 |null              |Cardio Machine Accessories       |Clarksville|Tennessee |credit      |
|1     |26/05/2011|4006742|198.44|Exercise & Fitness|Weightlifting Gloves             |Long Beach |California|credit      |
|2     |01/06/2011|4009775|5.58  |Exercise & Fitness|Weightlifting Machine Accessories|Anaheim    |California|credit      |
|3     |05/06/2011|4002199|198.19|Gymnastics        |Gymnastics Rings                 |Milwaukee  |Wisconsin |credit      |
|4     |17/12/2011|4002613|98.81 |Team Sports       |Field Hockey                     |Nashville  |Tennessee |credit      |
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+
only showing top 5 rows


===========================================================================================================
# scenario 7 convert string column to date column and apply a filter 
# convert the txn_date from string to date and list the data where txn_date > '30/09/2011'
#methond 1
df_txn.withColumn("txn_date", to_date(df_txn["txn_date"], "MM-dd-yyyy")) \
	.filter("txn_date > '2011-09-30'") \
	.sort("txn_date").show(5, truncate=False)
	
+------+----------+-------+------+------------------+--------------+--------------+----------+------------+
|txn_id|txn_date  |cust_id|amount|category          |product       |city          |state     |payment_type|
+------+----------+-------+------+------------------+--------------+--------------+----------+------------+
|1299  |2011-10-01|4004004|66.64 |Outdoor Recreation|Skateboarding |Las Vegas     |Nevada    |credit      |
|1583  |2011-10-01|4001947|170.47|Outdoor Recreation|Fishing       |Orange        |California|credit      |
|781   |2011-10-01|4003064|154.68|Water Sports      |Kitesurfing   |Gilbert       |Arizona   |credit      |
|1353  |2011-10-01|4004866|110.99|Racquet Sports    |Squash        |Sacramento    |California|credit      |
|1084  |2011-10-01|4009449|162.58|Outdoor Recreation|Shooting Games|St. Petersburg|Florida   |credit      |
+------+----------+-------+------+------------------+--------------+--------------+----------+------------+
only showing top 5 rows

#method 2 
df_txn.withColumn("txn_date", date_format(to_date(df_txn["txn_date"], "MM-dd-yyyy"), "dd/MM/yyyy")) \
	.filter("to_date(txn_date,'dd/MM/yyyy') > '2011-09-30'") \
	.sort(to_date("txn_date",'dd/MM/yyyy')).show(5, truncate=False)
	
+------+----------+-------+------+------------------+--------------+--------------+----------+------------+
|txn_id|txn_date  |cust_id|amount|category          |product       |city          |state     |payment_type|
+------+----------+-------+------+------------------+--------------+--------------+----------+------------+
|1299  |2011-10-01|4004004|66.64 |Outdoor Recreation|Skateboarding |Las Vegas     |Nevada    |credit      |
|1583  |2011-10-01|4001947|170.47|Outdoor Recreation|Fishing       |Orange        |California|credit      |
|781   |2011-10-01|4003064|154.68|Water Sports      |Kitesurfing   |Gilbert       |Arizona   |credit      |
|1353  |2011-10-01|4004866|110.99|Racquet Sports    |Squash        |Sacramento    |California|credit      |
|1084  |2011-10-01|4009449|162.58|Outdoor Recreation|Shooting Games|St. Petersburg|Florida   |credit      |
+------+----------+-------+------+------------------+--------------+--------------+----------+------------+
only showing top 5 rows

===========================================================================================================

# scenario 8 find the date difference on txn_date from current_Date
df_txn.withColumn("txn_date", to_date(df_txn["txn_date"], "MM-dd-yyyy")) \
	  .withColumn("date_diff",datediff(current_date(),col("txn_date"))) \
	  .sort(to_date("txn_date", 'dd/MM/yyyy')).show(5, truncate=False)

+------+----------+-------+------+------------------+------------------------------+--------+----------+------------+---------+
|txn_id|txn_date  |cust_id|amount|category          |product                       |city    |state     |payment_type|date_diff|
+------+----------+-------+------+------------------+------------------------------+--------+----------+------------+---------+
|1462  |2011-01-01|4004446|29.7  |Team Sports       |Softball                      |Pasadena|California|cash        |4616     |
|3198  |2011-01-01|4002352|126.8 |Team Sports       |Cricket                       |Everett |Washington|credit      |4616     |
|2171  |2011-01-01|4006131|198.2 |Outdoor Recreation|Camping & Backpacking & Hiking|El Paso |Texas     |credit      |4616     |
|107   |2011-01-01|4006017|177.45|Games             |Dice & Dice Sets              |Orange  |California|credit      |4616     |
|2249  |2011-01-01|4004935|68.89 |Water Sports      |Scuba Diving & Snorkeling     |Columbus|Georgia   |credit      |4616     |
+------+----------+-------+------+------------------+------------------------------+--------+----------+------------+---------+
only showing top 5 rows

# scenario 9 find the month difference on txn_date from current_Date
df_txn.withColumn("txn_date", to_date(df_txn["txn_date"], "MM-dd-yyyy")) \
	  .withColumn("months_diff",months_between(current_date(),col("txn_date"))) \
	  .sort(to_date("txn_date", 'dd/MM/yyyy')).show(5, truncate=False)
	  
+------+----------+-------+------+------------------+------------------------------+--------+----------+------------+------------+
|txn_id|txn_date  |cust_id|amount|category          |product                       |city    |state     |payment_type|months_diff |
+------+----------+-------+------+------------------+------------------------------+--------+----------+------------+------------+
|1462  |2011-01-01|4004446|29.7  |Team Sports       |Softball                      |Pasadena|California|cash        |151.67741935|
|3198  |2011-01-01|4002352|126.8 |Team Sports       |Cricket                       |Everett |Washington|credit      |151.67741935|
|2171  |2011-01-01|4006131|198.2 |Outdoor Recreation|Camping & Backpacking & Hiking|El Paso |Texas     |credit      |151.67741935|
|107   |2011-01-01|4006017|177.45|Games             |Dice & Dice Sets              |Orange  |California|credit      |151.67741935|
|2249  |2011-01-01|4004935|68.89 |Water Sports      |Scuba Diving & Snorkeling     |Columbus|Georgia   |credit      |151.67741935|
+------+----------+-------+------+------------------+------------------------------+--------+----------+------------+------------+
only showing top 5 rows

df_txn.withColumn("txn_date", to_date(df_txn["txn_date"], "MM-dd-yyyy")) \
          .withColumn("months_diff",round(months_between(current_date(),col("txn_date")),2)) \
          .sort(to_date("txn_date", 'dd/MM/yyyy')).show(5, truncate=False)
+------+----------+-------+------+------------------+------------------------------+--------+----------+------------+-----------+
|txn_id|txn_date  |cust_id|amount|category          |product                       |city    |state     |payment_type|months_diff|
+------+----------+-------+------+------------------+------------------------------+--------+----------+------------+-----------+
|1462  |2011-01-01|4004446|29.7  |Team Sports       |Softball                      |Pasadena|California|cash        |151.68     |
|3198  |2011-01-01|4002352|126.8 |Team Sports       |Cricket                       |Everett |Washington|credit      |151.68     |
|2171  |2011-01-01|4006131|198.2 |Outdoor Recreation|Camping & Backpacking & Hiking|El Paso |Texas     |credit      |151.68     |
|107   |2011-01-01|4006017|177.45|Games             |Dice & Dice Sets              |Orange  |California|credit      |151.68     |
|2249  |2011-01-01|4004935|68.89 |Water Sports      |Scuba Diving & Snorkeling     |Columbus|Georgia   |credit      |151.68     |
+------+----------+-------+------+------------------+------------------------------+--------+----------+------------+-----------+

Task 2
======
Explore the difference between CreateOrReplaceTempView and CreateGlobalTempView

What is the difference between CreateOrReplaceTempView and CreateGlobalTempView
answer : createOrReplaceTempView has been introduced in Spark 2.0 to replace registerTempTable. CreateTempView creates an in-memory reference to the 
Dataframe in use. The lifetime for this depends on the spark session in which the Dataframe was created in. createGlobalTempView, on the other hand, 
allows you to create the references that can be used across spark sessions. So depending upon whether you need to share data across sessions, you can 
use either of the methods. By default, the notebooks in the same cluster share the same spark session, but there is an option to set up clusters where 
each notebook has its own session. So all it boils down to is that where do you create the data frame and where do you want to access it.

What is create or replace temp view in PySpark?
The createOrReplaceTempView() is used to create a temporary view/table from the PySpark DataFrame or Dataset objects. Since it is a temporary view, 
the lifetime of the table/view is tied to the current SparkSession. Hence, It will be automatically removed when your SparkSession ends.

What is global temporary view?
Global temporary views are primarily used to share within a session and then get automatically dropped once the session ends. These session-scoped views 
also serve as a temporary table on which SQL queries can be made and are stored in database global_temp.

What is the difference between temp view and global temp view?
TEMPORARY views are session-scoped and will be dropped when session ends because it skips persisting the definition in the underlying metastore, if any. 
GLOBAL TEMPORARY views are tied to a system preserved temporary database global_temp. 

Give me the usecase where CreateOrReplaceTempView or CreateGlobalTempView

https://itecnote.com/tecnote/apache-spark-spark-createorreplacetempview-vs-createglobaltempview/

https://www.edureka.co/community/2229/difference-createorreplacetempview-registertemptable

a. Create the dataframe from txn file and create the new column txndate1 ( default date format) ==dftxn 
create a new dataframe from txn_dataframe , add the below column and populate the data
1) dayonmonth (txndate1) --> DAY
2) month  (txndate1) --> MONTH 
3) year  (txndate1) --> YEAR 

df_txn_1= df_txn.withColumn("txn_date_1", to_date(df_txn["txn_date"], "MM-dd-yyyy"))
df_txn_1.withColumn("DayOfMonth",dayofweek(col("txn_date_1"))) \
	.withColumn("Month", month(col("txn_date_1"))) \
	.withColumn("Year", year(col("txn_date_1"))) \
	.sort(to_date("txn_date", 'dd/MM/yyyy')).show(5, truncate=False)
		
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+----------+-----+----+
|txn_id|txn_date  |cust_id|amount|category          |product                          |city       |state     |payment_type|txn_date_1|DayOfMonth|Month|Year|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+----------+-----+----+
|0     |06-26-2011|4007024|40.33 |null              |Cardio Machine Accessories       |Clarksville|Tennessee |credit      |2011-06-26|1         |6    |2011|
|1     |05-26-2011|4006742|198.44|Exercise & Fitness|Weightlifting Gloves             |Long Beach |California|credit      |2011-05-26|5         |5    |2011|
|2     |06-01-2011|4009775|5.58  |Exercise & Fitness|Weightlifting Machine Accessories|Anaheim    |California|credit      |2011-06-01|4         |6    |2011|
|3     |06-05-2011|4002199|198.19|Gymnastics        |Gymnastics Rings                 |Milwaukee  |Wisconsin |credit      |2011-06-05|1         |6    |2011|
|4     |12-17-2011|4002613|98.81 |Team Sports       |Field Hockey                     |Nashville  |Tennessee |credit      |2011-12-17|7         |12   |2011|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+----------+-----+----+
only showing top 5 rows

b.Create the dataframe from txn file and create the new column txndate1 ( default date format) ==dftxn
you need to create 2 new columns 
1) ADDED_DATE --> ON TXNDATE1 --> 2 --> 25 --> 27  --
2) SUB_date --> ON txndate1 --> 2 --> 25 --> 23 

df_txn_1= df_txn.withColumn("txn_date_1", to_date(df_txn["txn_date"], "MM-dd-yyyy"))
df_txn_1.withColumn("txn_date+2",date_add(col("txn_date_1"),2)) \
        .withColumn("txn_date-2", date_sub(col("txn_date_1"), 2)) \
        .sort(to_date("txn_date", 'dd/MM/yyyy')).show(5, truncate=False)

+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+----------+----------+
|txn_id|txn_date  |cust_id|amount|category          |product                          |city       |state     |payment_type|txn_date_1|txn_date+2|txn_date-2|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+----------+----------+
|0     |06-26-2011|4007024|40.33 |null              |Cardio Machine Accessories       |Clarksville|Tennessee |credit      |2011-06-26|2011-06-28|2011-06-24|
|1     |05-26-2011|4006742|198.44|Exercise & Fitness|Weightlifting Gloves             |Long Beach |California|credit      |2011-05-26|2011-05-28|2011-05-24|
|2     |06-01-2011|4009775|5.58  |Exercise & Fitness|Weightlifting Machine Accessories|Anaheim    |California|credit      |2011-06-01|2011-06-03|2011-05-30|
|3     |06-05-2011|4002199|198.19|Gymnastics        |Gymnastics Rings                 |Milwaukee  |Wisconsin |credit      |2011-06-05|2011-06-07|2011-06-03|
|4     |12-17-2011|4002613|98.81 |Team Sports       |Field Hockey                     |Nashville  |Tennessee |credit      |2011-12-17|2011-12-19|2011-12-15|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+----------+----------+
only showing top 5 rows

or the same can be achieved using the below by passing negative values to get the revert functionality

df_txn_1= df_txn.withColumn("txn_date_1", to_date(df_txn["txn_date"], "MM-dd-yyyy"))
df_txn_1.withColumn("txn_date+2",date_sub(col("txn_date_1"),-2)) \
        .withColumn("txn_date-2", date_add(col("txn_date_1"), -2)) \
        .sort(to_date("txn_date", 'dd/MM/yyyy')).show(5, truncate=False)


+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+----------+----------+
|txn_id|txn_date  |cust_id|amount|category          |product                          |city       |state     |payment_type|txn_date_1|txn_date+2|txn_date-2|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+----------+----------+
|0     |06-26-2011|4007024|40.33 |null              |Cardio Machine Accessories       |Clarksville|Tennessee |credit      |2011-06-26|2011-06-28|2011-06-24|
|1     |05-26-2011|4006742|198.44|Exercise & Fitness|Weightlifting Gloves             |Long Beach |California|credit      |2011-05-26|2011-05-28|2011-05-24|
|2     |06-01-2011|4009775|5.58  |Exercise & Fitness|Weightlifting Machine Accessories|Anaheim    |California|credit      |2011-06-01|2011-06-03|2011-05-30|
|3     |06-05-2011|4002199|198.19|Gymnastics        |Gymnastics Rings                 |Milwaukee  |Wisconsin |credit      |2011-06-05|2011-06-07|2011-06-03|
|4     |12-17-2011|4002613|98.81 |Team Sports       |Field Hockey                     |Nashville  |Tennessee |credit      |2011-12-17|2011-12-19|2011-12-15|
+------+----------+-------+------+------------------+---------------------------------+-----------+----------+------------+----------+----------+----------+
only showing top 5 rows

c. Create the dataframe from txn file and create the new column txndate1 ( default date format) ==dftxn
txndate1 --> date 
convert txndate1 to timestamo column 
to_timestamp col

df_txn_1= df_txn.withColumn("txn_date_1", to_date(df_txn["txn_date"], "MM-dd-yyyy"))
df_txn_1.withColumn("txn_date_1", to_timestamp("txn_date_1")) \
        .sort("txn_date_1","txn_id").show(5, truncate=False)
		
+------+----------+-------+------+------------------+----------------+---------+----------+------------+-------------------+
|txn_id|txn_date  |cust_id|amount|category          |product         |city     |state     |payment_type|txn_date_1         |
+------+----------+-------+------+------------------+----------------+---------+----------+------------+-------------------+
|107   |01-01-2011|4006017|177.45|Games             |Dice & Dice Sets|Orange   |California|credit      |2011-01-01 00:00:00|
|281   |01-01-2011|4009900|181.6 |Gymnastics        |Balance Beams   |Madison  |Wisconsin |credit      |2011-01-01 00:00:00|
|1082  |01-01-2011|4002800|132.07|Exercise & Fitness|Foam Rollers    |Irving   |Texas     |credit      |2011-01-01 00:00:00|
|1317  |01-01-2011|4007516|142.92|Winter Sports     |Snowboarding    |Vancouver|Washington|credit      |2011-01-01 00:00:00|
|1462  |01-01-2011|4004446|29.7  |Team Sports       |Softball        |Pasadena |California|cash        |2011-01-01 00:00:00|
+------+----------+-------+------+------------------+----------------+---------+----------+------------+-------------------+
only showing top 5 rows

==========================================================================================================================================
Handling Complex Data in pyspark
================================

Complex data refers schema less data, example json or xml files.

processing JSON file (Java script object notation)
====================

JSON documents consists of key value pair elements organized with using {},",",[]

1. Simple JSON documents
========================
When a JSON document doesn't have any array data or nested struct data is called simple JSON.

Examples for simple JSON
sample 1
========
{
  "device_id": 1,
  "device_name": "sensor-mac-xztr9HZ4Z6YT",
  "humidity": 36,
  "long": 56,
  "scale": "Celius",
  "temp": 14,
  "timestamp": 1447975124.005187,
  "zipcode": 96484
}
sample 2
========
{
  "id": "001",
  "type": "cake",
  "name": "Cream",
  "image": {
    "url": "images/0001jpg",
    "width": 200,
    "height": 201
  },
  "thumbnail": {
    "url": "images/thumbnails/0002.jpg",
    "width": 34,
    "height": 33
  }
}

how to process simple json documents?
Sample 1
========
    # ###reading simple json
    df_simp_json = spark.read.format("json") \
	      .option("multiline" , False)\
          .load("file:///D:/Practising/PySpark/DATASETS/devices.json")
    print('No of records in the json file ',df_simp_json.count())
    df_simp_json.printSchema()
    df_simp_json.show(truncate=False)
	
	
Sample Data in the file "devices.json"	
{"device_id": 1, "device_name": "sensor-mac-xztr9HZ4Z6YT", "humidity": 36,"long": 56, "scale": "Celius", "temp": 14, "timestamp": 1447975124.005187, "zipcode": 96484}
{"device_id": 2, "device_name": "sensor-mac-able9bI4h6kR", "humidity": 36, "lat": 13, "long": 56, "scale": "Celius", "temp": 14, "timestamp": 1447975124.005187, "zipcode": 96484}
{"device_id": 3, "device_name": "sensor-mac-aboutRPHN0rak", "humidity": 30, "lat": 35, "long": 54, "scale": "Celius", "temp": 17, "timestamp": 1447975124.054221, "zipcode": 96402}
{"device_id": 4, "device_name": "sensor-mac-acrossSIdxLScj", "humidity": 35, "lat": 36, "long": 40, "scale": "Celius", "temp": 6, "timestamp": 1447975124.102157, "zipcode": 96025}
{"device_id": 5, "device_name": "sensor-mac-afterEJOOMANT", "humidity": 62, "lat": 90, "long": 91, "scale": "Celius", "temp": 23, "timestamp": 1447975124.150419, "zipcode": 95638}
{"device_id": 6, "device_name": "sensor-mac-allBMOfeOuD", "humidity": 97, "lat": 76, "long": 77, "scale": "Celius", "temp": 8, "timestamp": 1447975124.197694, "zipcode": 95478}
{"device_id": 7, "device_name": "sensor-mac-almostRYxyBNj9", "humidity": 30, "lat": 95, "long": 76, "scale": "Celius", "temp": 5, "timestamp": 1447975124.246103, "zipcode": 95499}
{"device_id": 8, "device_name": "sensor-mac-alsosTDq3ePw", "humidity": 74, "lat": 95, "long": 79, "scale": "Celius", "temp": 25, "timestamp": 1447975124.291892, "zipcode": 94857}
{"device_id": 9, "device_name": "sensor-mac-am8CUlnzss", "humidity": 93, "lat": 37, "long": 29, "scale": "Celius", "temp": 24, "timestamp": 1447975124.349377, "zipcode": 96149}
{"device_id": 10, "device_name": "sensor-mac-amongj68Lqjia", "humidity": 67, "lat": 32, "long": 31, "scale": "Celius", "temp": 6, "timestamp": 1447975124.399264, "zipcode": 96531}

Note: When the data(entire record) is organized in the same line, then make sure to add the option ("multiline",False)
output
======

No of records in the json file  322
root
 |-- device_id: long (nullable = true)
 |-- device_name: string (nullable = true)
 |-- humidity: long (nullable = true)
 |-- lat: long (nullable = true)
 |-- long: long (nullable = true)
 |-- scale: string (nullable = true)
 |-- temp: long (nullable = true)
 |-- timestamp: double (nullable = true)
 |-- zipcode: long (nullable = true)

+---------+--------------------------+--------+----+----+------+----+-------------------+-------+
|device_id|device_name               |humidity|lat |long|scale |temp|timestamp          |zipcode|
+---------+--------------------------+--------+----+----+------+----+-------------------+-------+
|1        |sensor-mac-xztr9HZ4Z6YT   |36      |null|56  |Celius|14  |1.447975124005187E9|96484  |
|2        |sensor-mac-able9bI4h6kR   |36      |13  |56  |Celius|14  |1.447975124005187E9|96484  |
|3        |sensor-mac-aboutRPHN0rak  |30      |35  |54  |Celius|17  |1.447975124054221E9|96402  |
|4        |sensor-mac-acrossSIdxLScj |35      |36  |40  |Celius|6   |1.447975124102157E9|96025  |
|5        |sensor-mac-afterEJOOMANT  |62      |90  |91  |Celius|23  |1.447975124150419E9|95638  |
|6        |sensor-mac-allBMOfeOuD    |97      |76  |77  |Celius|8   |1.447975124197694E9|95478  |
|7        |sensor-mac-almostRYxyBNj9 |30      |95  |76  |Celius|5   |1.447975124246103E9|95499  |
|8        |sensor-mac-alsosTDq3ePw   |74      |95  |79  |Celius|25  |1.447975124291892E9|94857  |
|9        |sensor-mac-am8CUlnzss     |93      |37  |29  |Celius|24  |1.447975124349377E9|96149  |
|10       |sensor-mac-amongj68Lqjia  |67      |32  |31  |Celius|6   |1.447975124399264E9|96531  |
|11       |sensor-mac-anKbFxnWkc     |77      |68  |44  |Celius|8   |1.447975124448621E9|95850  |
|12       |sensor-mac-andeKYc1g3v    |96      |98  |87  |Celius|23  |1.447975124498138E9|95524  |
|13       |sensor-mac-anyf4C4ssHP    |54      |78  |14  |Celius|33  |1.447975124546061E9|96025  |
|14       |sensor-mac-arevx9V9q1o    |83      |64  |90  |Celius|29  |1.447975124596101E9|95470  |
|15       |sensor-mac-asZWypbFbU     |89      |6   |70  |Celius|16  |1.447975124641159E9|96756  |
|16       |sensor-mac-atV9GYdopQ     |73      |73  |13  |Celius|32  |1.447975124685129E9|96846  |
|17       |sensor-mac-be8Ab7Mq5a     |53      |29  |54  |Celius|25  |1.447975124734084E9|95323  |
|18       |sensor-mac-becausebs1k83V1|97      |8   |60  |Celius|15  |1.447975124783517E9|96577  |
|19       |sensor-mac-beenTil0UmrA   |62      |67  |72  |Celius|31  |1.447975124832391E9|96358  |
|20       |sensor-mac-butYnx5AWrQ    |29      |57  |16  |Celius|5   |1.447975124881703E9|96669  |
+---------+--------------------------+--------+----+----+------+----+-------------------+-------+
only showing top 20 rows

Sample 2
========
    ##sample for simple json with multiple struct column    #
    df_comp_json = spark.read.format("json") \
        .option("multiline" , False)\
        .load("file:///D:/Practising/PySpark/DATASETS/jsn_str_original.json")
    print('No of records in the json file ',df_comp_json.count())
    df_comp_json.printSchema()
    df_comp_json.show(truncate=False)
	
Sample Data from jsn_str.json

{"id": "001","type": "cake","name": "Cream","image": {"url": "images/0001jpg","width": 200,"height": 201},"thumbnail": {"url": "images/thumbnails/0002.jpg","width": 34,"height": 33}}
{"id": "002","type": "bun cake","name": "Cream","image": {"url": "images/0003jpg","width": 200,"height": 201},"thumbnail": {"url": "images/thumbnails/0002.jpg","width": 34,"height": 33}}
{"id": "003","type": "pastry cake","name": "Cream","image": {"url": "images/0004jpg","width": 200,"height": 201},"thumbnail": {"url": "images/thumbnails/0002.jpg","width": 35,"height": 45}}
{"id": "004","type": "sweet cake","name": "Cream","image": {"url": "images/0005jpg","width": 200,"height": 201},"thumbnail": {"url": "images/thumbnails/0002.jpg","width": 36,"height": 47}}

output 
======
No of records in the json file  4
root
 |-- id: string (nullable = true)
 |-- image: struct (nullable = true)
 |    |-- height: long (nullable = true)
 |    |-- url: string (nullable = true)
 |    |-- width: long (nullable = true)
 |-- name: string (nullable = true)
 |-- thumbnail: struct (nullable = true)
 |    |-- height: long (nullable = true)
 |    |-- url: string (nullable = true)
 |    |-- width: long (nullable = true)
 |-- type: string (nullable = true)

+---+--------------------------+-----+------------------------------------+-----------+
|id |image                     |name |thumbnail                           |type       |
+---+--------------------------+-----+------------------------------------+-----------+
|001|{201, images/0001jpg, 200}|Cream|{33, images/thumbnails/0002.jpg, 34}|cake       |
|002|{201, images/0003jpg, 200}|Cream|{33, images/thumbnails/0002.jpg, 34}|bun cake   |
|003|{201, images/0004jpg, 200}|Cream|{45, images/thumbnails/0002.jpg, 35}|pastry cake|
|004|{201, images/0005jpg, 200}|Cream|{47, images/thumbnails/0002.jpg, 36}|sweet cake |
+---+--------------------------+-----+------------------------------------+-----------+

to select the individual columns from a struct element use .notation, in the above example
image and thumbnail fields are struct type and they have subfields
to display/access the subfields use struct_field_name.sub_field_name.

image.height or thumbnail.width, if we want to display all the columns from the struct field use struct_field_name.*


	df_comp_json.select("id","name","type","image.url","thumbnail.*") \
             .filter(col("thumbnail.width") >= 35).show(truncate=False)

+---+-----+-----------+--------------+------+--------------------------+-----+
|id |name |type       |url           |height|url                       |width|
+---+-----+-----------+--------------+------+--------------------------+-----+
|003|Cream|pastry cake|images/0004jpg|45    |images/thumbnails/0002.jpg|35   |
|004|Cream|sweet cake |images/0005jpg|47    |images/thumbnails/0002.jpg|36   |
+---+-----+-----------+--------------+------+--------------------------+-----+


Formatted JSON files
use the link https://jsonformatter.org/json-parser#google_vignette to format the json files.
When the json file is formated then each key item will be there in a separate line, know if you don't set the option ("multiline",True"), 
it will throw the error.

 # # #sample for reading formatted json file
    df_comp_json = spark.read.format("json") \
        .option("multiline", False)\
        .load("file:///D:/Practising/PySpark/DATASETS/jsn_str_formatted.json")
    print('No of records in the json file ',df_comp_json.count())
    df_comp_json.printSchema()
    df_comp_json.show(truncate=False)
	
output
======
Error	

No of records in the json file  61
root
 |-- _corrupt_record: string (nullable = true)

Traceback (most recent call last):
  File "D:\Practising\PySpark\Project01\Batch14_PySpark_Core\Complex_Data_Processing.py", line 36, in <module>
    df_comp_json.show(truncate=False)
  File "C:\Users\Windows 10\PySparkLib\spark-3.1.3-bin-hadoop3.2\python\lib\pyspark.zip\pyspark\sql\dataframe.py", line 486, in show
  File "C:\Users\Windows 10\PySparkLib\spark-3.1.3-bin-hadoop3.2\python\lib\py4j-0.10.9-src.zip\py4j\java_gateway.py", line 1304, in __call__
  File "C:\Users\Windows 10\PySparkLib\spark-3.1.3-bin-hadoop3.2\python\lib\pyspark.zip\pyspark\sql\utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the
referenced columns only include the internal corrupt record column
(named _corrupt_record by default). For example:
spark.read.schema(schema).json(file).filter($"_corrupt_record".isNotNull).count()
and spark.read.schema(schema).json(file).select("_corrupt_record").show().
Instead, you can cache or save the parsed results and then send the same query.
For example, val df = spark.read.schema(schema).json(file).cache() and then
df.filter($"_corrupt_record".isNotNull).count().

now changed the option "multiline" to True.

Note: when the JSON is formatted and if it has multiple records then it should be in a [] each record is separated by a ","

 # # #sample for reading formatted json file
    df_comp_json = spark.read.format("json") \
        .option("multiline", False)\
        .load("file:///D:/Practising/PySpark/DATASETS/jsn_str_formatted.json")
    print('No of records in the json file ',df_comp_json.count())
    df_comp_json.printSchema()
    df_comp_json.show(truncate=False)
	
No of records in the json file  4
root
 |-- id: string (nullable = true)
 |-- image: struct (nullable = true)
 |    |-- height: long (nullable = true)
 |    |-- url: string (nullable = true)
 |    |-- width: long (nullable = true)
 |-- name: string (nullable = true)
 |-- thumbnail: struct (nullable = true)
 |    |-- height: long (nullable = true)
 |    |-- url: string (nullable = true)
 |    |-- width: long (nullable = true)
 |-- type: string (nullable = true)

+---+--------------------------+-----+------------------------------------+-----------+
|id |image                     |name |thumbnail                           |type       |
+---+--------------------------+-----+------------------------------------+-----------+
|001|{201, images/0001jpg, 200}|Cream|{33, images/thumbnails/0002.jpg, 34}|cake       |
|002|{201, images/0003jpg, 200}|Cream|{33, images/thumbnails/0002.jpg, 34}|bun cake   |
|003|{201, images/0004jpg, 200}|Cream|{45, images/thumbnails/0002.jpg, 35}|pastry cake|
|004|{201, images/0005jpg, 200}|Cream|{47, images/thumbnails/0002.jpg, 36}|sweet cake |
+---+--------------------------+-----+------------------------------------+-----------+

	
2. Complex JSON documents
=========================

when the JSON contains multiple nested struct or nested string fields it become complex.

Sample for nested struct fields
===============================
[
{
	"Country": "Australia",
	"User": {
		"name": {
			"first_name" : "Nanda",
			"middle_name" : "Kumar",
			"last_name" : "Govindan"
		},
		"address": {
			"unit":"8",
			"number": "39",
			"street": "Ross Street",
			"suburb" : "North Parramatta",
			"city": "Sydney",
			"state": "NSW",	
			"postcode": "2151"
		},
		"Age":42,
		"phone":"+61405891583"
	}
},
{
	"Country": "Australia",
	"User": {
		"name": {
			"first_name" : "Ganesh",
			"middle_name" : "Murugan",
			"last_name": "Paramasivan"
		},
		"address": {
			"number": "24",
			"street": "Campbell Street",
			"suburb": "Parramatta",
			"postcode": "2150"
		},
		"Age":34
	}
}
]

    df_comp_json = spark.read.format("json") \
    .option("multiline" , True)\
    .load("file:///D:/Practising/PySpark/DATASETS/place.json")
    print(df_comp_json.count())
    df_comp_json.printSchema()
    df_comp_json.show(truncate=False)
	
output
======

No of records in the json file  2
root
 |-- Country: string (nullable = true)
 |-- User: struct (nullable = true)
 |    |-- Age: long (nullable = true)
 |    |-- address: struct (nullable = true)
 |    |    |-- city: string (nullable = true)
 |    |    |-- number: string (nullable = true)
 |    |    |-- postcode: string (nullable = true)
 |    |    |-- state: string (nullable = true)
 |    |    |-- street: string (nullable = true)
 |    |    |-- suburb: string (nullable = true)
 |    |    |-- unit: string (nullable = true)
 |    |-- name: struct (nullable = true)
 |    |    |-- first_name: string (nullable = true)
 |    |    |-- last_name: string (nullable = true)
 |    |    |-- middle_name: string (nullable = true)
 |    |-- phone: string (nullable = true)

+---------+-------------------------------------------------------------------------------------------------------+
|Country  |User                                                                                                   |
+---------+-------------------------------------------------------------------------------------------------------+
|Australia|{42, {Sydney, 39, 2151, NSW, Ross Street, North Parramatta, 8}, {Nanda, Govindan, Kumar}, +61405891583}|
|Australia|{34, {null, 24, 2150, null, Campbell Street, Parramatta, null}, {Ganesh, Paramasivan, Murugan}, null}  |
+---------+-------------------------------------------------------------------------------------------------------+

accessing columns from the struct fields

    df_comp_json.select("user.name.first_name","user.name.last_name",\
                        "user.address.city","user.address.postcode",\
                        "user.phone","country")\
                        .sort("user.name.first_name").show()

+----------+-----------+------+--------+------------+---------+
|first_name|  last_name|  city|postcode|       phone|  country|
+----------+-----------+------+--------+------------+---------+
|    Ganesh|Paramasivan|  null|    2150|        null|Australia|
|     Nanda|   Govindan|Sydney|    2151|+61405891583|Australia|
+----------+-----------+------+--------+------------+---------+


Sample for nested array
=======================
[
  {
    "Id": 1,
    "Country": "Australia",
    "User": {
      "name": {
        "first_name": "Nanda",
        "middle_name": "Kumar",
        "last_name": "Govindan"
      },
      "address": {
        "unit": "8",
        "number": "39",
        "street": "Ross Street",
        "suburb": "North Parramatta",
        "city": "Sydney",
        "state": "NSW",
        "postcode": "2151"
      },
      "Age": 42,
      "phone": "+61405891583",
      "skills": [
        {
          "technology": "Oracle",
          "experience": 14
        },
        {
          "technology": "MSSQL",
          "experience": 4
        },
        {
          "technology": "MySQL",
          "experience": 2
        },
        {
          "technology": "Snowflake",
          "experience": 0.5
        },
        {
          "technology": "Forms & Reports",
          "experience": 8
        }
      ]
    },
    "email": "nanda.gnk@gmail.com"
  },
  {
    "Id": 2,
    "Country": "Australia",
    "User": {
      "name": {
        "first_name": "Ganesh",
        "middle_name": "Murugan",
        "last_name": "Paramasivan"
      },
      "address": {
        "number": "24",
        "street": "Campbell Street",
        "suburb": "Parramatta",
        "postcode": "2150"
      },
      "Age": 34,
      "skills": [
        {
          "technology": "Oracle",
          "experience": 4
        },
        {
          "technology": "MSSQL",
          "experience": 12
        },
        {
          "technology": "MySQL",
          "experience": 4
        },
        {
          "technology": "Snowflake",
          "experience": 0.5
        },
        {
          "technology": "SSIS",
          "experience": 5
        }
      ]
    },
    "email": "ganesh.m@xyz.com"
  }
]

processing nested array json

    df_comp_json = spark.read.format("json") \
    .option("multiline" , True)\
    .load("file:///D:/Practising/PySpark/DATASETS/profile_info.json")
    print('No of records in the json file ', df_comp_json.count())
    df_comp_json.printSchema()
    df_comp_json.show(truncate= False)


output
======
No of records in the json file  2
root
 |-- Country: string (nullable = true)
 |-- Id: long (nullable = true)
 |-- User: struct (nullable = true)
 |    |-- Age: long (nullable = true)
 |    |-- address: struct (nullable = true)
 |    |    |-- city: string (nullable = true)
 |    |    |-- number: string (nullable = true)
 |    |    |-- postcode: string (nullable = true)
 |    |    |-- state: string (nullable = true)
 |    |    |-- street: string (nullable = true)
 |    |    |-- suburb: string (nullable = true)
 |    |    |-- unit: string (nullable = true)
 |    |-- name: struct (nullable = true)
 |    |    |-- first_name: string (nullable = true)
 |    |    |-- last_name: string (nullable = true)
 |    |    |-- middle_name: string (nullable = true)
 |    |-- phone: string (nullable = true)
 |    |-- skills: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- experience: double (nullable = true)
 |    |    |    |-- technology: string (nullable = true)
 |-- email: string (nullable = true)

+---------+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+
|Country  |Id |User                                                                                                                                                                                           |email              |
+---------+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+
|Australia|1  |{42, {Sydney, 39, 2151, NSW, Ross Street, North Parramatta, 8}, {Nanda, Govindan, Kumar}, +61405891583, [{14.0, Oracle}, {4.0, MSSQL}, {2.0, MySQL}, {0.5, Snowflake}, {8.0, Forms & Reports}]}|nanda.gnk@gmail.com|
|Australia|2  |{34, {null, 24, 2150, null, Campbell Street, Parramatta, null}, {Ganesh, Paramasivan, Murugan}, null, [{4.0, Oracle}, {12.0, MSSQL}, {4.0, MySQL}, {0.5, Snowflake}, {5.0, SSIS}]}             |ganesh.m@xyz.com   |
+---------+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+

Explode
=======

Note: To convert the array of elements present in the rows into columns use explode function, but only one explode is allowed per select.

    df_comp_json.select("id","user.name.first_name",\
        explode(df_comp_json["user.skills.experience"]).alias("Exp")).show()
		
output
======
+---+--------------------+----+
| id|user.name.first_name| Exp|
+---+--------------------+----+
|  1|               Nanda|14.0|
|  1|               Nanda| 4.0|
|  1|               Nanda| 2.0|
|  1|               Nanda| 0.5|
|  1|               Nanda| 8.0|
|  2|              Ganesh| 4.0|
|  2|              Ganesh|12.0|
|  2|              Ganesh| 4.0|
|  2|              Ganesh| 0.5|
|  2|              Ganesh| 5.0|
+---+--------------------+----+

Generating a sequence number for each row for "Experience".

    df_comp_json.select("id",col("user.name.first_name").alias("Name"),\
         explode(df_comp_json["user.skills.experience"]).alias("Experience")) \
         .withColumn("seq_no",monotonically_increasing_id()).show()

	df2=df_comp_json.select("id",col("user.name.first_name").alias("Name"),\
         explode(df_comp_json["user.skills.experience"]).alias("experience")) \
         .withColumn("seq_no",monotonically_increasing_id())			 
		 
output 
======
No of records in the json file  2
+---+------+----------+------+
| id|  Name|Experience|seq_no|
+---+------+----------+------+
|  1| Nanda|      14.0|     0|
|  1| Nanda|       4.0|     1|
|  1| Nanda|       2.0|     2|
|  1| Nanda|       0.5|     3|
|  1| Nanda|       8.0|     4|
|  2|Ganesh|       4.0|     5|
|  2|Ganesh|      12.0|     6|
|  2|Ganesh|       4.0|     7|
|  2|Ganesh|       0.5|     8|
|  2|Ganesh|       5.0|     9|
+---+------+----------+------+

similarly generating another df for technology

    df_comp_json.select("id",col("user.name.first_name").alias("Name"),\
         explode(df_comp_json["user.skills.technology"]).alias("technology")) \
         .withColumn("seq_no",monotonically_increasing_id()).show()
		 
    df1=df_comp_json.select("id",col("user.name.first_name").alias("Name"),\
         explode(df_comp_json["user.skills.technology"]).alias("technology")) \
         .withColumn("seq_no",monotonically_increasing_id())	 
    
output
====== 

+---+------+---------------+------+
| id|  Name|     technology|seq_no|
+---+------+---------------+------+
|  1| Nanda|         Oracle|     0|
|  1| Nanda|          MSSQL|     1|
|  1| Nanda|          MySQL|     2|
|  1| Nanda|      Snowflake|     3|
|  1| Nanda|Forms & Reports|     4|
|  2|Ganesh|         Oracle|     5|
|  2|Ganesh|          MSSQL|     6|
|  2|Ganesh|          MySQL|     7|
|  2|Ganesh|      Snowflake|     8|
|  2|Ganesh|           SSIS|     9|
+---+------+---------------+------+

now joining these two dataframe to get the technology and experience in a signle df by joining id,name and seq_no.

The reason for introduing seq_no into the above dataframe is to get a unique join key

Now join the two dataframe to get the required result set.

    df1.join(df2, ["id","Name","seq_no"],"inner")\
        .drop("seq_no").orderBy("id","experience").show()
		
output
======
+---+------+---------------+----------+
| id|  Name|     technology|experience|
+---+------+---------------+----------+
|  1| Nanda|      Snowflake|       0.5|
|  1| Nanda|          MySQL|       2.0|
|  1| Nanda|          MSSQL|       4.0|
|  1| Nanda|Forms & Reports|       8.0|
|  1| Nanda|         Oracle|      14.0|
|  2|Ganesh|      Snowflake|       0.5|
|  2|Ganesh|          MySQL|       4.0|
|  2|Ganesh|         Oracle|       4.0|
|  2|Ganesh|           SSIS|       5.0|
|  2|Ganesh|          MSSQL|      12.0|
+---+------+---------------+----------+

applying a filter condition

    df1.join(df2, ["id","Name","seq_no"],"inner")\
        .filter(df2.experience>=5)\
        .drop("seq_no").show()

No of records in the json file  2
+---+------+---------------+----------+
| id|  Name|     technology|experience|
+---+------+---------------+----------+
|  1| Nanda|         Oracle|      14.0|
|  1| Nanda|Forms & Reports|       8.0|
|  2|Ganesh|          MSSQL|      12.0|
|  2|Ganesh|           SSIS|       5.0|
+---+------+---------------+----------+

join this with the main dataframe and get the remaining columns 		

    df3=df_comp_json.select(col("Id").alias("id"),\
                            col("country"),col("email"),\
                            col("user.age").alias("age"),\
                            col("user.address.city").alias("city"),\
                            col("user.phone").alias("phone"))
							
    df1.join(df2, ["id", "Name", "seq_no"], "inner") \
        .filter(df2.experience >= 5) \
        .join(df3, "id", "inner") \
        .select(df1.id,df1.Name,df3.age,\
                df1.technology,df2.experience,\
                df3.city,df3.phone,df3.email).show()

No of records in the json file  2
+---+------+---+---------------+----------+------+------------+-------------------+
| id|  Name|age|     technology|experience|  city|       phone|              email|
+---+------+---+---------------+----------+------+------------+-------------------+
|  1| Nanda| 42|         Oracle|      14.0|Sydney|+61405891583|nanda.gnk@gmail.com|
|  1| Nanda| 42|Forms & Reports|       8.0|Sydney|+61405891583|nanda.gnk@gmail.com|
|  2|Ganesh| 34|          MSSQL|      12.0|  null|        null|   ganesh.m@xyz.com|
|  2|Ganesh| 34|           SSIS|       5.0|  null|        null|   ganesh.m@xyz.com|
+---+------+---+---------------+----------+------+------------+-------------------+

	##========================================================##
    ##This can be achieved in much simpler way refer the below##
    ##========================================================##
    
    df = df_comp_json.select ("*",explode(df_comp_json["user.skills"]).alias("skills"))
    ###This will convert the array column skills into a struct column
    df.printSchema()

root
 |-- Country: string (nullable = true)
 |-- Id: long (nullable = true)
 |-- User: struct (nullable = true)
 |    |-- Age: long (nullable = true)
 |    |-- address: struct (nullable = true)
 |    |    |-- city: string (nullable = true)
 |    |    |-- number: string (nullable = true)
 |    |    |-- postcode: string (nullable = true)
 |    |    |-- state: string (nullable = true)
 |    |    |-- street: string (nullable = true)
 |    |    |-- suburb: string (nullable = true)
 |    |    |-- unit: string (nullable = true)
 |    |-- name: struct (nullable = true)
 |    |    |-- first_name: string (nullable = true)
 |    |    |-- last_name: string (nullable = true)
 |    |    |-- middle_name: string (nullable = true)
 |    |-- phone: string (nullable = true)
 |    |-- skills: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- experience: double (nullable = true)
 |    |    |    |-- technology: string (nullable = true)
 |-- email: string (nullable = true)
 |-- skills: struct (nullable = true)
 |    |-- experience: double (nullable = true)
 |    |-- technology: string (nullable = true)
 

	##now we can easily query each element in the struct field
    df.select("id",col("user.name.first_name").alias("Name"),\
              col("user.age").alias("age"),\
              col("user.address.city").alias("city"),\
              col("user.phone").alias("phone_no"), \
              col("email").alias("mail_id"),\
              col("skills.technology").alias("Technology"),\
              col("skills.experience").alias("Year_of_Experience")).show(truncate=False)

output
======			  
+---+------+---+------+------------+-------------------+---------------+------------------+
|id |Name  |age|city  |phone_no    |mail_id            |Technology     |Year_of_Experience|
+---+------+---+------+------------+-------------------+---------------+------------------+
|1  |Nanda |42 |Sydney|+61405891583|nanda.gnk@gmail.com|Oracle         |14.0              |
|1  |Nanda |42 |Sydney|+61405891583|nanda.gnk@gmail.com|MSSQL          |4.0               |
|1  |Nanda |42 |Sydney|+61405891583|nanda.gnk@gmail.com|MySQL          |2.0               |
|1  |Nanda |42 |Sydney|+61405891583|nanda.gnk@gmail.com|Snowflake      |0.5               |
|1  |Nanda |42 |Sydney|+61405891583|nanda.gnk@gmail.com|Forms & Reports|8.0               |
|2  |Ganesh|34 |null  |null        |ganesh.m@xyz.com   |Oracle         |4.0               |
|2  |Ganesh|34 |null  |null        |ganesh.m@xyz.com   |MSSQL          |12.0              |
|2  |Ganesh|34 |null  |null        |ganesh.m@xyz.com   |MySQL          |4.0               |
|2  |Ganesh|34 |null  |null        |ganesh.m@xyz.com   |Snowflake      |0.5               |
|2  |Ganesh|34 |null  |null        |ganesh.m@xyz.com   |SSIS           |5.0               |
+---+------+---+------+------------+-------------------+---------------+------------------+

	## now we can easily query each element in the struct field with filter condition
    df.filter(col("skills.experience")>=5)\
      .select("id", col("user.name.first_name").alias("Name"), \
              col("user.age").alias("age"), \
              col("user.address.city").alias("city"), \
              col("user.phone").alias("phone_no"), \
              col("email").alias("mail_id"), \
              col("skills.technology").alias("Technology"), \
              col("skills.experience").alias("Year_of_Experience")).show(truncate=False)
			  
output
======			  
+---+------+---+------+------------+-------------------+---------------+------------------+
|id |Name  |age|city  |phone_no    |mail_id            |Technology     |Year_of_Experience|
+---+------+---+------+------------+-------------------+---------------+------------------+
|1  |Nanda |42 |Sydney|+61405891583|nanda.gnk@gmail.com|Oracle         |14.0              |
|1  |Nanda |42 |Sydney|+61405891583|nanda.gnk@gmail.com|Forms & Reports|8.0               |
|2  |Ganesh|34 |null  |null        |ganesh.m@xyz.com   |MSSQL          |12.0              |
|2  |Ganesh|34 |null  |null        |ganesh.m@xyz.com   |SSIS           |5.0               |
+---+------+---+------+------------+-------------------+---------------+------------------+

####===================================================================================####
	##selecing the nested array elements using index
    df_comp_json.select(df_comp_json["id"],\
                        df_comp_json["user.name.first_name"],\
                        df_comp_json["user.skills"][0],\
                        df_comp_json["user.skills"][1],\
                        df_comp_json["user.skills"][2],\
                        df_comp_json["user.skills"][3],\
                        df_comp_json["user.skills"][4]).show()

output
======
+---+----------+--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+
| id|first_name|user.skills AS `skills`[0]|user.skills AS `skills`[1]|user.skills AS `skills`[2]|user.skills AS `skills`[3]|user.skills AS `skills`[4]|
+---+----------+--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+
|  1|     Nanda|            {14.0, Oracle}|              {4.0, MSSQL}|              {2.0, MySQL}|          {0.5, Snowflake}|      {8.0, Forms & Rep...|
|  2|    Ganesh|             {4.0, Oracle}|             {12.0, MSSQL}|              {4.0, MySQL}|          {0.5, Snowflake}|               {5.0, SSIS}|
+---+----------+--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+

Task for Complex JSON Data
==========================

Take the file Array_Complex.JSON

{
	"results": [{
		"gender": "male",
		"name": {
			"title": "mr",
			"first": "brad",
			"last": "gibson"
		},
		"location": {
			"street": "9278 new road",
			"city": "kilcoole",
			"state": "waterford",
			"postcode": "93027",
			"coordinates": {
				"latitude": "20.9267",
				"longitude": "-7.9310"
			},
			"timezone": {
				"offset": "-3:30",
				"description": "Newfoundland"
			}
		},
		"email": "brad.gibson@example.com",
		"login": {
			"uuid": "155e77ee-ba6d-486f-95ce-0e0c0fb4b919",
			"username": "silverswan131",
			"password": "firewall",
			"salt": "TQA1Gz7x",
			"md5": "dc523cb313b63dfe5be2140b0c05b3bc",
			"sha1": "7a4aa07d1bedcc6bcf4b7f8856643492c191540d",
			"sha256": "74364e96174afa7d17ee52dd2c9c7a4651fe1254f471a78bda0190135dcd3480"
		},
		"dob": {
			"date": "1993-07-20T09:44:18.674Z",
			"age": 26
		},
		"registered": {
			"date": "2002-05-21T10:59:49.966Z",
			"age": 17
		},
		"phone": "011-962-7516",
		"cell": "081-454-0666",
		"id": {
			"name": "PPS",
			"value": "0390511T"
		},
		"picture": {
			"large": "https://randomuser.me/api/portraits/men/75.jpg",
			"medium": "https://randomuser.me/api/portraits/med/men/75.jpg",
			"thumbnail": "https://randomuser.me/api/portraits/thumb/men/75.jpg"
		},
		"nat": "IE"
	}],
	"info": {
		"seed": "fea8be3e64777240",
		"results": 1,
		"page": 1,
		"version": "1.3"
	}
}

Task:
Convert this json into a Dataframe, Identify the array data and flatten the Array data

    ### Task for processig complex json with nested array data
    df_comp_json = spark.read.format("json") \
        .option("multiline", True) \
        .load("file:///D:/Practising/PySpark/DATASETS/array_complex.json")
    print('No of records in the json file ', df_comp_json.count())
    df_comp_json.printSchema()
    df_comp_json.show(truncate= False)

No of records in the json file  1
root
 |-- info: struct (nullable = true)
 |    |-- page: long (nullable = true)
 |    |-- results: long (nullable = true)
 |    |-- seed: string (nullable = true)
 |    |-- version: string (nullable = true)
 |-- results: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- cell: string (nullable = true)
 |    |    |-- dob: struct (nullable = true)
 |    |    |    |-- age: long (nullable = true)
 |    |    |    |-- date: string (nullable = true)
 |    |    |-- email: string (nullable = true)
 |    |    |-- gender: string (nullable = true)
 |    |    |-- id: struct (nullable = true)
 |    |    |    |-- name: string (nullable = true)
 |    |    |    |-- value: string (nullable = true)
 |    |    |-- location: struct (nullable = true)
 |    |    |    |-- city: string (nullable = true)
 |    |    |    |-- coordinates: struct (nullable = true)
 |    |    |    |    |-- latitude: string (nullable = true)
 |    |    |    |    |-- longitude: string (nullable = true)
 |    |    |    |-- postcode: string (nullable = true)
 |    |    |    |-- state: string (nullable = true)
 |    |    |    |-- street: string (nullable = true)
 |    |    |    |-- timezone: struct (nullable = true)
 |    |    |    |    |-- description: string (nullable = true)
 |    |    |    |    |-- offset: string (nullable = true)
 |    |    |-- login: struct (nullable = true)
 |    |    |    |-- md5: string (nullable = true)
 |    |    |    |-- password: string (nullable = true)
 |    |    |    |-- salt: string (nullable = true)
 |    |    |    |-- sha1: string (nullable = true)
 |    |    |    |-- sha256: string (nullable = true)
 |    |    |    |-- username: string (nullable = true)
 |    |    |    |-- uuid: string (nullable = true)
 |    |    |-- name: struct (nullable = true)
 |    |    |    |-- first: string (nullable = true)
 |    |    |    |-- last: string (nullable = true)
 |    |    |    |-- title: string (nullable = true)
 |    |    |-- nat: string (nullable = true)
 |    |    |-- phone: string (nullable = true)
 |    |    |-- picture: struct (nullable = true)
 |    |    |    |-- large: string (nullable = true)
 |    |    |    |-- medium: string (nullable = true)
 |    |    |    |-- thumbnail: string (nullable = true)
 |    |    |-- registered: struct (nullable = true)
 |    |    |    |-- age: long (nullable = true)
 |    |    |    |-- date: string (nullable = true)

+-----------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|info                         |results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
+-----------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|{1, 1, fea8be3e64777240, 1.3}|[{081-454-0666, {26, 1993-07-20T09:44:18.674Z}, brad.gibson@example.com, male, {PPS, 0390511T}, {kilcoole, {20.9267, -7.9310}, 93027, waterford, 9278 new road, {Newfoundland, -3:30}}, {dc523cb313b63dfe5be2140b0c05b3bc, firewall, TQA1Gz7x, 7a4aa07d1bedcc6bcf4b7f8856643492c191540d, 74364e96174afa7d17ee52dd2c9c7a4651fe1254f471a78bda0190135dcd3480, silverswan131, 155e77ee-ba6d-486f-95ce-0e0c0fb4b919}, {brad, gibson, mr}, IE, 011-962-7516, {https://randomuser.me/api/portraits/men/75.jpg, https://randomuser.me/api/portraits/med/men/75.jpg, https://randomuser.me/api/portraits/thumb/men/75.jpg}, {17, 2002-05-21T10:59:49.966Z}}]|
+-----------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+


    df1= df_comp_json.select("info.*", explode(df_comp_json.results).alias("results_exploded"))
    df1.printSchema()

No of records in the json file  1
root
 |-- page: long (nullable = true)
 |-- results: long (nullable = true)
 |-- seed: string (nullable = true)
 |-- version: string (nullable = true)
 |-- results_exploded: struct (nullable = true)
 |    |-- cell: string (nullable = true)
 |    |-- dob: struct (nullable = true)
 |    |    |-- age: long (nullable = true)
 |    |    |-- date: string (nullable = true)
 |    |-- email: string (nullable = true)
 |    |-- gender: string (nullable = true)
 |    |-- id: struct (nullable = true)
 |    |    |-- name: string (nullable = true)
 |    |    |-- value: string (nullable = true)
 |    |-- location: struct (nullable = true)
 |    |    |-- city: string (nullable = true)
 |    |    |-- coordinates: struct (nullable = true)
 |    |    |    |-- latitude: string (nullable = true)
 |    |    |    |-- longitude: string (nullable = true)
 |    |    |-- postcode: string (nullable = true)
 |    |    |-- state: string (nullable = true)
 |    |    |-- street: string (nullable = true)
 |    |    |-- timezone: struct (nullable = true)
 |    |    |    |-- description: string (nullable = true)
 |    |    |    |-- offset: string (nullable = true)
 |    |-- login: struct (nullable = true)
 |    |    |-- md5: string (nullable = true)
 |    |    |-- password: string (nullable = true)
 |    |    |-- salt: string (nullable = true)
 |    |    |-- sha1: string (nullable = true)
 |    |    |-- sha256: string (nullable = true)
 |    |    |-- username: string (nullable = true)
 |    |    |-- uuid: string (nullable = true)
 |    |-- name: struct (nullable = true)
 |    |    |-- first: string (nullable = true)
 |    |    |-- last: string (nullable = true)
 |    |    |-- title: string (nullable = true)
 |    |-- nat: string (nullable = true)
 |    |-- phone: string (nullable = true)
 |    |-- picture: struct (nullable = true)
 |    |    |-- large: string (nullable = true)
 |    |    |-- medium: string (nullable = true)
 |    |    |-- thumbnail: string (nullable = true)
 |    |-- registered: struct (nullable = true)
 |    |    |-- age: long (nullable = true)
 |    |    |-- date: string (nullable = true)
 
 
    ### now we can easily query the required columns
    df1.select("*",\
               col("results_exploded.id.name").alias("Id_Name"), \
               col("results_exploded.id.value").alias("Id_Value"),\
               col("results_exploded.name.first").alias("First_Name"),\
               col("results_exploded.dob.date").alias("D.O.Birth"),\
               col("results_exploded.email").alias("Email"),\
               col("results_exploded.login.username").alias("Username"))\
       .drop("results_exploded").show(truncate=False)

+----+-------+----------------+-------+-------+--------+----------+------------------------+-----------------------+-------------+
|page|results|seed            |version|Id_Name|Id_Value|First_Name|D.O.Birth               |Email                  |Username     |
+----+-------+----------------+-------+-------+--------+----------+------------------------+-----------------------+-------------+
|1   |1      |fea8be3e64777240|1.3    |PPS    |0390511T|brad      |1993-07-20T09:44:18.674Z|brad.gibson@example.com|silverswan131|
+----+-------+----------------+-------+-------+--------+----------+------------------------+-----------------------+-------------+

https://sparkbyexamples.com/pyspark/pyspark-explode-array-and-map-columns-to-rows/#:~:text=PySpark%20function%20explode(e%3A%20Column,it%20contains%20all%20array%20elements.

https://saturncloud.io/blog/converting-multiple-array-of-structs-columns-in-pyspark-sql-a-comprehensive-guide/

============================================================================================================================
============================================================================================================================
Spark to RDBMS integration
==========================
Conneting to RDBMS from Spark

https://kontext.tech/article/1060/pyspark-read-data-from-oracle-database

Scenario connect to oracle read a table and perform transactions on the table and store the result into MySql database.

    ####Source Systerm details####
    sql = "select * from emp"
    source_user = "HR"
    source_pwd = "hratoraclexe"
    # Change this to your Oracle's details accordingly
    source_server = "DESKTOP-E4HB1LH"
    source_port = 1521
    source_service_name = "XE"
    source_jdbcUrl = f"jdbc:oracle:thin:@{source_server}:{source_port}/{source_service_name}"
    source_jdbcDriver = "oracle.jdbc.driver.OracleDriver"

    ####Destination Systerm details####
    dest_user = "devdbuser"
    dest_pwd = "DevDBUser@2023"
    # Change this to your Oracle's details accordingly
    dest_server = "DESKTOP-E4HB1LH"
    dest_port = 3306
    dest_db_name = "devdb"
    dest_jdbcUrl = f"jdbc:mysql://{dest_server}:{dest_port}/{dest_db_name}"
    dest_jdbcDriver = "com.mysql.cj.jdbc.Driver"

    df_oracle = spark.read.format("jdbc")\
        .option("url",source_jdbcUrl) \
        .option("query", sql) \
        .option("user",source_user) \
        .option("password", source_pwd) \
        .option("driver", source_jdbcDriver).load()

    #df_oracle.printSchema()

    print("No of records in the table : ",df_oracle.count())

    ###applying transformation on the df
    df_result=df_oracle.withColumn("Name",concat("first_name",lit(" "),"last_name").alias("Name"))\
                .withColumn("Join_Date",date_format("hire_date","dd/MM/yyyy"))\
                .select("employee_id","name","Job_id","join_date","salary","commission_pct","department_id")\
                .filter((col("salary")>5000) & (col("commission_pct").isNull()) & (col("department_id") == 50))\
                .sort("join_date")

    ###Writing the result to mysql db
    df_result.write.format("jdbc")\
             .mode("overwrite") \
             .option("url", dest_jdbcUrl) \
             .option("dbtable", "emp_result") \
             .option("user", dest_user) \
             .option("password", dest_pwd) \
             .option("driver", dest_jdbcDriver).save()
			 
output
======
connect to MySQL
use devdb;
select * from emp_result;

122	Payam Kaufling	ST_MAN	01/05/2003	7900.00		50
121	Adam Fripp	    ST_MAN	10/04/2005	8200.00		50
123	Shanta Vollman	ST_MAN	10/10/2005	6500.00		50
124	Kevin Mourgos	ST_MAN	16/11/2007	5800.00		50
120	Matthew Weiss	ST_MAN	18/07/2004	8000.00		50

===================================================================================================================
===================================================================================================================
==========================
Spark to HIVE integaration
==========================

=======================================
How to access hive tables? from PySpark
=======================================

In Hive there is a table cust_nanda

open a terminal in clouldxlab and check the table cust_nanda

hive> 
    > select count(1) from cust_nanda;
Query ID = nandagnk2141_20230831133015_423468f0-565c-4ce8-9d0a-46c1cfd931ed
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1648130833540_33004, Tracking URL = http://cxln2.c.thelab-240901.internal:8088/proxy/application_1648130833540_33004/
Kill Command = /usr/hdp/2.6.2.0-205/hadoop/bin/hadoop job  -kill job_1648130833540_33004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-08-31 13:30:27,851 Stage-1 map = 0%,  reduce = 0%
2023-08-31 13:30:35,499 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.2 sec
2023-08-31 13:30:45,185 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.67 sec
MapReduce Total cumulative CPU time: 5 seconds 670 msec
Ended Job = job_1648130833540_33004
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.67 sec   HDFS Read: 963833 HDFS Write: 6 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 670 msec
OK
12440
Time taken: 31.338 seconds, Fetched: 1 row(s)

it took 31.338 seconds the get the count of records in the table from hive.

hive> desc formatted cust_nanda
    > ;
OK
# col_name              data_type               comment             
                 
customer_id             int                                         
customer_fname          string                                      
customer_lname          string                                      
customer_email          string                                      
customer_password       string                                      
customer_street         string                                      
customer_city           string                                      
customer_state          string                                      
customer_zipcode        string                                      
                 
# Detailed Table Information             
Database:               default                  
Owner:                  nandagnk2141             
CreateTime:             Sun Nov 13 13:31:26 UTC 2022     
LastAccessTime:         UNKNOWN                  
Protect Mode:           None                     
Retention:              0                        
Location:               hdfs://cxln1.c.thelab-240901.internal:8020/apps/hive/warehouse/cust_nanda        
Table Type:             MANAGED_TABLE            
Table Parameters:                
        comment                 Imported by sqoop on 2022/11/13 13:31:21
        numFiles                4                   
        numRows                 0                   
        rawDataSize             0                   
        totalSize               954017              
        transient_lastDdlTime   1668346286          
                 
# Storage Information            
SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       
InputFormat:            org.apache.hadoop.mapred.TextInputFormat         
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       
Compressed:             No                       
Num Buckets:            -1                       
Bucket Columns:         []                       
Sort Columns:           []                       
Storage Desc Params:             
        field.delim             \u0001              
        line.delim              \n                  
        serialization.format    \u0001              
Time taken: 0.081 seconds, Fetched: 41 row(s)

now, I'm going to access the table from pyspark

open an another terminal from cloudxlab and connect to pyspark

[nandagnk2141@cxln5 ~]$ pyspark
SPARK_MAJOR_VERSION is set to 2, using Spark2
Python 2.7.5 (default, Apr  9 2019, 14:30:50) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.1.1.2.6.2.0-205
      /_/
Using Python version 2.7.5 (default, Apr  9 2019 14:30:50)
SparkSession available as 'spark'.
>>> df = spark.sql("select * from cust_nanda");
>>> df.printSchema()
root
 |-- customer_id: integer (nullable = true)
 |-- customer_fname: string (nullable = true)
 |-- customer_lname: string (nullable = true)
 |-- customer_email: string (nullable = true)
 |-- customer_password: string (nullable = true)
 |-- customer_street: string (nullable = true)
 |-- customer_city: string (nullable = true)
 |-- customer_state: string (nullable = true)
 |-- customer_zipcode: string (nullable = true
 
>>> df.count()
12440                                                                           

>>> df.show(10,truncate=False)
+-----------+--------------+--------------+--------------+-----------------+---------------------------+-------------+--------------+----------------+
|customer_id|customer_fname|customer_lname|customer_email|customer_password|customer_street            |customer_city|customer_state|customer_zipcode|
+-----------+--------------+--------------+--------------+-----------------+---------------------------+-------------+--------------+----------------+
|1          |Richard       |Hernandez     |XXXXXXXXX     |XXXXXXXXX        |6303 Heather Plaza         |Brownsville  |TX            |78521           |
|2          |Mary          |Barrett       |XXXXXXXXX     |XXXXXXXXX        |9526 Noble Embers Ridge    |Littleton    |CO            |80126           |
|3          |Ann           |Smith         |XXXXXXXXX     |XXXXXXXXX        |3422 Blue Pioneer Bend     |Caguas       |PR            |00725           |
|4          |Mary          |Jones         |XXXXXXXXX     |XXXXXXXXX        |8324 Little Common         |San Marcos   |CA            |92069           |
|5          |Robert        |Hudson        |XXXXXXXXX     |XXXXXXXXX        |10 Crystal River Mall      |Caguas       |PR            |00725           |
|6          |Mary          |Smith         |XXXXXXXXX     |XXXXXXXXX        |3151 Sleepy Quail Promenade|Passaic      |NJ            |07055           |
|7          |Melissa       |Wilcox        |XXXXXXXXX     |XXXXXXXXX        |9453 High Concession       |Caguas       |PR            |00725           |
|8          |Megan         |Smith         |XXXXXXXXX     |XXXXXXXXX        |3047 Foggy Forest Plaza    |Lawrence     |MA            |01841           |
|9          |Mary          |Perez         |XXXXXXXXX     |XXXXXXXXX        |3616 Quaking Street        |Caguas       |PR            |00725           |
|10         |Melissa       |Smith         |XXXXXXXXX     |XXXXXXXXX        |8598 Harvest Beacon Plaza  |Stafford     |VA            |22554           |
+-----------+--------------+--------------+--------------+-----------------+---------------------------+-------------+--------------+----------------+
only showing top 10 rows

pyspark is returning the count in fraction of seconds compare to hive.

>>> df.filter(col("customer_city") == "Stafford")\
... .select("customer_id","customer_fname","customer_street","customer_city","customer_state","customer_zipcode").show(10,truncate=False)
+-----------+--------------+-------------------------+-------------+--------------+----------------+
|customer_id|customer_fname|customer_street          |customer_city|customer_state|customer_zipcode|
+-----------+--------------+-------------------------+-------------+--------------+----------------+
|10         |Melissa       |8598 Harvest Beacon Plaza|Stafford     |VA            |22554           |
|691        |Mary          |3310 Blue Quay           |Stafford     |VA            |22554           |
|3610       |Jordan        |2355 Amber Panda Vista   |Stafford     |VA            |22554           |
|4260       |Debra         |6772 Broad Fawn Park     |Stafford     |VA            |22554           |
|6036       |Barbara       |938 Crystal Leaf Beach   |Stafford     |VA            |22554           |
|6577       |Mary          |1165 Quiet Inlet         |Stafford     |VA            |22554           |
|7482       |Mary          |40 Gentle Zephyr Moor    |Stafford     |VA            |22554           |
|12410      |Melissa       |8598 Harvest Beacon Plaza|Stafford     |VA            |22554           |
|12420      |Melissa       |8598 Harvest Beacon Plaza|Stafford     |VA            |22554           |
|12430      |Melissa       |8598 Harvest Beacon Plaza|Stafford     |VA            |22554           |
+-----------+--------------+-------------------------+-------------+--------------+----------------+
only showing top 10 rows

======================
writing data into hive
======================

read the data from txn_records and apply transformation and save the result back to hive.
>>> df_txn=spark.sql("select * from txn_records");
>>> df_txn.printSchema()
root
 |-- key: integer (nullable = true)
 |-- txndate: string (nullable = true)
 |-- custno: integer (nullable = true)
 |-- amount: double (nullable = true)
 |-- category: string (nullable = true)
 |-- product: string (nullable = true)
 |-- city: string (nullable = true)
 |-- state: string (nullable = true)
 |-- spendby: string (nullable = true)
>>> df_txn.count()
95904

>>> df_txn.show(10,truncate=False)
+---+----------+-------+------+----------------------+---------------------------------+--------------+--------------+-------+
|key|txndate   |custno |amount|category              |product                          |city          |state         |spendby|
+---+----------+-------+------+----------------------+---------------------------------+--------------+--------------+-------+
|0  |06-26-2011|4007024|40.33 |                      |Cardio Machine Accessories       |Clarksville   |Tennessee     |credit |
|1  |05-26-2011|4006742|198.44|Exercise & Fitness    |Weightlifting Gloves             |Long Beach    |California    |credit |
|2  |06-01-2011|4009775|5.58  |Exercise & Fitness    |Weightlifting Machine Accessories|Anaheim       |California    |credit |
|3  |06-05-2011|4002199|198.19|Gymnastics            |Gymnastics Rings                 |Milwaukee     |Wisconsin     |credit |
|4  |12-17-2011|4002613|98.81 |Team Sports           |Field Hockey                     |Nashville     |Tennessee     |credit |
|5  |02-14-2011|4007591|193.63|Outdoor Recreation    |Camping & Backpacking & Hiking   |Chicago       |Illinois      |credit |
|6  |10-28-2011|4002190|27.89 |Puzzles               |Jigsaw Puzzles                   |Charleston    |South Carolina|credit |
|7  |07-14-2011|4002964|96.01 |Outdoor Play Equipment|Sandboxes                        |Columbus      |Ohio          |credit |
|8  |01-17-2011|4007361|10.44 |Winter Sports         |Snowmobiling                     |Des Moines    |Iowa          |credit |
|9  |05-17-2011|4004798|152.46|Jumping               |Bungee Jumping                   |St. Petersburg|Florida       |credit |
+---+----------+-------+------+----------------------+---------------------------------+--------------+--------------+-------+
only showing top 10 rows

>>> df_txn1 = df_txn.filter(((col("state") == "Florida") & (col("category")=="Jumping")) | (col("amount")>200))\
... .select("key","txndate","custno","amount","product","spendby")
>>> df_txn1.count()
177
>>> df_txn1.printSchema()
root
 |-- key: integer (nullable = true)
 |-- txndate: string (nullable = true)
 |-- custno: integer (nullable = true)
 |-- amount: double (nullable = true)
 |-- product: string (nullable = true)
 |-- spendby: string (nullable = true)

>>> df_txn1.write.format("parquet").mode("overwrite").saveAsTable("gnanda80.txn_records_parquet")

Note: When you write a table in hive from spark, it will be created as internal table.

now check the table in hive.

hive> desc txn_records_parquet;
OK
key                     int                                         
txndate                 string                                      
custno                  int                                         
amount                  double                                      
product                 string                                      
spendby                 string                                      
Time taken: 0.079 seconds, Fetched: 6 row(s)
hive> select count(1) from txn_records_parquet;
Query ID = nandagnk2141_20230831141057_473e90fd-d157-4e00-9d6c-ac154f0d41df
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1648130833540_33006, Tracking URL = http://cxln2.c.thelab-240901.internal:8088/proxy/application_1648130833540_33006/
Kill Command = /usr/hdp/2.6.2.0-205/hadoop/bin/hadoop job  -kill job_1648130833540_33006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-08-31 14:11:07,418 Stage-1 map = 0%,  reduce = 0%
2023-08-31 14:11:14,694 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.86 sec
2023-08-31 14:11:21,958 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.7 sec
MapReduce Total cumulative CPU time: 7 seconds 700 msec
Ended Job = job_1648130833540_33006
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.7 sec   HDFS Read: 15304 HDFS Write: 4 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 700 msec
OK
177
Time taken: 25.606 seconds, Fetched: 1 row(s)
hive> desc formatted txn_records_parquet;
OK
# col_name              data_type               comment             
                 
key                     int                                         
txndate                 string                                      
custno                  int                                         
amount                  double                                      
  set hive.exec.reducers.max=<number>
product                 string                                      
spendby                 string                                      
                 
# Detailed Table Information             
Database:               gnanda80                 
Owner:                  nandagnk2141             
CreateTime:             Thu Aug 31 14:09:45 UTC 2023     
LastAccessTime:         UNKNOWN                  
Protect Mode:           None                     
Retention:              0                        
Location:               hdfs://cxln1.c.thelab-240901.internal:8020/apps/hive/warehouse/gnanda80.db/txn_records_parquet   
Table Type:             MANAGED_TABLE            
Table Parameters:                
        numFiles                1                   
        spark.sql.sources.provider      parquet             
        spark.sql.sources.schema.numParts       1                   
        spark.sql.sources.schema.part.0 {\"type\":\"struct\",\"fields\":[{\"name\":\"key\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"txndate\"
,\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"custno\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"amount\",\"type\":\"dou
ble\",\"nullable\":true,\"metadata\":{}},{\"name\":\"product\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"spendby\",\"type\":\"string\",\"nullab
le\":true,\"metadata\":{}}]}
        totalSize               4840                
        transient_lastDdlTime   1693490985          
                 
# Storage Information            
SerDe Library:          org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe      
InputFormat:            org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat    
OutputFormat:           org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat   
Compressed:             No                       
Num Buckets:            -1                       
Bucket Columns:         []                       
Sort Columns:           []                       
Storage Desc Params:             
        path                    hdfs://cxln1.c.thelab-240901.internal:8020/apps/hive/warehouse/gnanda80.db/txn_records_parquet
        serialization.format    1                   
Time taken: 0.172 seconds, Fetched: 37 row(s)

hive> select * from txn_records_parquet limit 5;
OK
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
9       05-17-2011      4004798 152.46  Bungee Jumping  credit
1856    05-18-2011      4003378 91.7    Bungee Jumping  credit
1954    01-06-2011      4006835 59.27   Trampoline Accessories  credit
2056    01-13-2011      4002727 99.57   Trampolines     credit
2083    11-15-2011      4000041 108.56  Trampoline Accessories  credit
Time taken: 0.058 seconds, Fetched: 5 row(s)

=======================================
now, write the file in orc file format.
=======================================

>>> df_txn1.write.format("orc").mode("overwrite").saveAsTable("gnanda80.txn_records_orc")

hive> desc txn_records_orc;
OK
key                     int                                         
txndate                 string                                      
custno                  int                                         
amount                  double                                      
product                 string                                      
spendby                 string                                      
Time taken: 0.092 seconds, Fetched: 6 row(s)
hive> select count(1) from txn_records_orc;
Query ID = nandagnk2141_20230831141839_19b1eac3-e998-4c53-b119-e2474d4e25ee
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1648130833540_33007, Tracking URL = http://cxln2.c.thelab-240901.internal:8088/proxy/application_1648130833540_33007/
Kill Command = /usr/hdp/2.6.2.0-205/hadoop/bin/hadoop job  -kill job_1648130833540_33007
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-08-31 14:18:45,265 Stage-1 map = 0%,  reduce = 0%
2023-08-31 14:18:52,542 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.73 sec
2023-08-31 14:18:57,728 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.85 sec
MapReduce Total cumulative CPU time: 6 seconds 850 msec
Ended Job = job_1648130833540_33007
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 6.85 sec   HDFS Read: 15307 HDFS Write: 4 SUCCESS
Total MapReduce CPU Time Spent: 6 seconds 850 msec
OK
177
Time taken: 20.754 seconds, Fetched: 1 row(s)
hive> desc formatted txn_records_orc
    > ;
OK
# col_name              data_type               comment             
                 
key                     int                                         
txndate                 string                                      
custno                  int                                         
amount                  double                                      
product                 string                                      
spendby                 string                                      
                 
# Detailed Table Information             
Database:               gnanda80                 
Owner:                  nandagnk2141             
CreateTime:             Thu Aug 31 14:17:41 UTC 2023     
LastAccessTime:         UNKNOWN                  
MapReduce Total cumulative CPU time: 6 seconds 850 msec
Protect Mode:           None                     
Retention:              0                        
Location:               hdfs://cxln1.c.thelab-240901.internal:8020/apps/hive/warehouse/gnanda80.db/txn_records_orc       
Table Type:             MANAGED_TABLE            
Table Parameters:                
        numFiles                1                   
        spark.sql.sources.provider      orc                 
        spark.sql.sources.schema.numParts       1                   
        spark.sql.sources.schema.part.0 {\"type\":\"struct\",\"fields\":[{\"name\":\"key\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"txndate\"
,\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"custno\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"amount\",\"type\":\"dou
ble\",\"nullable\":true,\"metadata\":{}},{\"name\":\"product\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"spendby\",\"type\":\"string\",\"nullab
le\":true,\"metadata\":{}}]}
        totalSize               3788                
        transient_lastDdlTime   1693491461          
                 
# Storage Information            
SerDe Library:          org.apache.hadoop.hive.ql.io.orc.OrcSerde        
InputFormat:            org.apache.hadoop.hive.ql.io.orc.OrcInputFormat  
OutputFormat:           org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat         
Compressed:             No                       
Num Buckets:            -1                       
Bucket Columns:         []                       
Sort Columns:           []                       
Storage Desc Params:             
        path                    hdfs://cxln1.c.thelab-240901.internal:8020/apps/hive/warehouse/gnanda80.db/txn_records_orc
        serialization.format    1                   
Time taken: 0.068 seconds, Fetched: 37 row(s)
hive> select * from txn_records_orc limit 5;
OK
9       05-17-2011      4004798 152.46  Bungee Jumping  credit
1856    05-18-2011      4003378 91.7    Bungee Jumping  credit
1954    01-06-2011      4006835 59.27   Trampoline Accessories  credit
2056    01-13-2011      4002727 99.57   Trampolines     credit
2083    11-15-2011      4000041 108.56  Trampoline Accessories  credit
Time taken: 0.046 seconds, Fetched: 5 row(s) 

=======================================
now, write the file in json file format.
=======================================
>>> df_txn1.write.format("json").mode("overwrite").saveAsTable("gnanda80.txn_records_json")
23/08/31 14:24:03 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider json. Persisting data source table `gnanda80`.`txn_records_j
son` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.

hive> desc txn_records_json;
OK
col                     array<string>           from deserializer   
Time taken: 0.072 seconds, Fetched: 1 row(s)

hive> desc formatted txn_records_json;
OK
# col_name              data_type               comment             
                 
col                     array<string>           from deserializer   
                 
# Detailed Table Information             
Database:               gnanda80                 
Owner:                  nandagnk2141             
CreateTime:             Thu Aug 31 14:24:03 UTC 2023     
LastAccessTime:         UNKNOWN                  
Protect Mode:           None                     
Retention:              0                        
Location:               hdfs://cxln1.c.thelab-240901.internal:8020/apps/hive/warehouse/gnanda80.db/txn_records_json      
Table Type:             MANAGED_TABLE            
Table Parameters:                
        numFiles                1                   
        spark.sql.sources.provider      json                
        spark.sql.sources.schema.numParts       1                   
        spark.sql.sources.schema.part.0 {\"type\":\"struct\",\"fields\":[{\"name\":\"key\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"txndate\"
,\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"custno\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"amount\",\"type\":\"dou
ble\",\"nullable\":true,\"metadata\":{}},{\"name\":\"product\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"spendby\",\"type\":\"string\",\"nullab
le\":true,\"metadata\":{}}]}
        totalSize               20508               
        transient_lastDdlTime   1693491843          
                 
# Storage Information            
SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       
InputFormat:            org.apache.hadoop.mapred.SequenceFileInputFormat         
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat        
Compressed:             No                       
Num Buckets:            -1                       
Bucket Columns:         []                       
Sort Columns:           []                       
Storage Desc Params:             
        path                    hdfs://cxln1.c.thelab-240901.internal:8020/apps/hive/warehouse/gnanda80.db/txn_records_json
        serialization.format    1                   
Time taken: 0.069 seconds, Fetched: 32 row(s)

hive won't support querying the data in JSON format.

hive> select * from txn_records_json;
OK
Failed with exception java.io.IOException:java.io.IOException: hdfs://cxln1.c.thelab-240901.internal:8020/apps/hive/warehouse/gnanda80.db/txn_records_json/part-00000-0c
c9e9d8-e4f6-4eec-a055-28f9b14d82d0.json not a SequenceFile
Time taken: 0.078 seconds
hive> 

but we can read the data from the file directly.

[nandagnk2141@cxln5 ~]$ hdfs dfs -cat /apps/hive/warehouse/gnanda80.db/txn_records_json/part-00000-0cc9e9d8-e4f6-4eec-a055-28f9b14d82d0.json
{"key":9,"txndate":"05-17-2011","custno":4004798,"amount":152.46,"product":"Bungee Jumping","spendby":"credit"}
{"key":1856,"txndate":"05-18-2011","custno":4003378,"amount":91.7,"product":"Bungee Jumping","spendby":"credit"}
{"key":1954,"txndate":"01-06-2011","custno":4006835,"amount":59.27,"product":"Trampoline Accessories","spendby":"credit"}
{"key":2056,"txndate":"01-13-2011","custno":4002727,"amount":99.57,"product":"Trampolines","spendby":"credit"}
{"key":2083,"txndate":"11-15-2011","custno":4000041,"amount":108.56,"product":"Trampoline Accessories","spendby":"credit"}
{"key":2135,"txndate":"10-10-2011","custno":4003243,"amount":167.93,"product":"Trampolines","spendby":"credit"}
{"key":2330,"txndate":"04-01-2011","custno":4006738,"amount":197.61,"product":"Pogo Sticks","spendby":"credit"}
{"key":2427,"txndate":"08-02-2011","custno":4004501,"amount":12.49,"product":"Trampolines","spendby":"credit"}
{"key":3036,"txndate":"11-27-2011","custno":4001780,"amount":146.18,"product":"Trampolines","spendby":"credit"}
{"key":4345,"txndate":"02-19-2011","custno":4006013,"amount":128.44,"product":"Jumping Stilts","spendby":"credit"}
{"key":4443,"txndate":"01-14-2011","custno":4009878,"amount":64.51,"product":"Trampolines","spendby":"credit"}
{"key":6024,"txndate":"09-09-2011","custno":4003668,"amount":40.79,"product":"Trampoline Accessories","spendby":"credit"}
{"key":6075,"txndate":"12-04-2011","custno":4002943,"amount":134.18,"product":"Jumping Stilts","spendby":"credit"}
{"key":6644,"txndate":"02-20-2011","custno":4005475,"amount":55.91,"product":"Trampolines","spendby":"credit"}
{"key":7311,"txndate":"12-13-2011","custno":4008093,"amount":191.3,"product":"Trampoline Accessories","spendby":"credit"}


>>> df_txn1.write.format("csv").mode("overwrite").saveAsTable("gnanda80.txn_records_csv")
23/08/31 14:37:13 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `gnanda80`.`txn_records_cs
v` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.
>>> 

CSV format also not supported by Hive.

=================================================================================================================
=================================================================================================================
==========================
Read JSON data from API.
==========================

sample API ->  https://randomuser.me/api

actual data in the api
{"results":[{"gender":"male","name":{"title":"Mr","first":"Emas","last":"Ferreira"},"location":{"street":{"number":9268,"name":"Rua So Luiz "},"city":"Uberaba","state":"Alagoas","country":"Brazil","postcode":31435,"coordinates":{"latitude":"-75.3650","longitude":"-113.7088"},"timezone":{"offset":"+3:00","description":"Baghdad, Riyadh, Moscow, St. Petersburg"}},"email":"emaus.ferreira@example.com","login":{"uuid":"578ece03-8ce5-4cd3-bb0d-cca98be67512","username":"angrysnake783","password":"printer","salt":"FX5tWZmE","md5":"68653b29cef026c387cc85d54ba2fb8f","sha1":"ff9e999de869130ee45dfe4dfebb6b3057e24753","sha256":"d828c8380abe092e98ee46e760ca14ddafd3062f9e730bf2b117982ae4d458b4"},"dob":{"date":"1990-06-14T12:44:44.501Z","age":33},"registered":{"date":"2014-12-18T16:35:39.182Z","age":8},"phone":"(57) 9343-0173","cell":"(59) 3248-3873","id":{"name":"CPF","value":"991.972.722-13"},"picture":{"large":"https://randomuser.me/api/portraits/men/62.jpg","medium":"https://randomuser.me/api/portraits/med/men/62.jpg","thumbnail":"https://randomuser.me/api/portraits/thumb/men/62.jpg"},"nat":"BR"}],"info":{"seed":"f9ce721abb23d265","results":1,"page":1,"version":"1.4"}}

formatted the data for better understanding
{
  "results": [
    {
      "gender": "male",
      "name": {
        "title": "Mr",
        "first": "Emas",
        "last": "Ferreira"
      },
      "location": {
        "street": {
          "number": 9268,
          "name": "Rua So Luiz "
        },
        "city": "Uberaba",
        "state": "Alagoas",
        "country": "Brazil",
        "postcode": 31435,
        "coordinates": {
          "latitude": "-75.3650",
          "longitude": "-113.7088"
        },
        "timezone": {
          "offset": "+3:00",
          "description": "Baghdad, Riyadh, Moscow, St. Petersburg"
        }
      },
      "email": "emaus.ferreira@example.com",
      "login": {
        "uuid": "578ece03-8ce5-4cd3-bb0d-cca98be67512",
        "username": "angrysnake783",
        "password": "printer",
        "salt": "FX5tWZmE",
        "md5": "68653b29cef026c387cc85d54ba2fb8f",
        "sha1": "ff9e999de869130ee45dfe4dfebb6b3057e24753",
        "sha256": "d828c8380abe092e98ee46e760ca14ddafd3062f9e730bf2b117982ae4d458b4"
      },
      "dob": {
        "date": "1990-06-14T12:44:44.501Z",
        "age": 33
      },
      "registered": {
        "date": "2014-12-18T16:35:39.182Z",
        "age": 8
      },
      "phone": "(57) 9343-0173",
      "cell": "(59) 3248-3873",
      "id": {
        "name": "CPF",
        "value": "991.972.722-13"
      },
      "picture": {
        "large": "https://randomuser.me/api/portraits/men/62.jpg",
        "medium": "https://randomuser.me/api/portraits/med/men/62.jpg",
        "thumbnail": "https://randomuser.me/api/portraits/thumb/men/62.jpg"
      },
      "nat": "BR"
    }
  ],
  "info": {
    "seed": "f9ce721abb23d265",
    "results": 1,
    "page": 1,
    "version": "1.4"
  }
}


how to read the data from API, let's see.

step 1 import urllib2
step 2 read the data as an object using the method urllib2.urlopen(URL)
step 3 create an RDD using the object created as part of step 2
step 4 create a dataframe from the RDD created in step 3.


import urllib2
contents = urllib2.urlopen("https://randomuser.me/api").read()
rdd = sc.parallelize([contents])
df = spark.read.json(rdd)
df.count()
df.printSchema()
df.show(truncate=False)


>>> import urllib2
>>> contents = urllib2.urlopen("https://randomuser.me/api").read()
>>> rdd = sc.parallelize([contents])
>>> df = spark.read.json(rdd)
>>> df.count()
1
>>> df.printSchema()
root
 |-- info: struct (nullable = true)
 |    |-- page: long (nullable = true)
 |    |-- results: long (nullable = true)
 |    |-- seed: string (nullable = true)
 |    |-- version: string (nullable = true)
 |-- results: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- cell: string (nullable = true)
 |    |    |-- dob: struct (nullable = true)
 |    |    |    |-- age: long (nullable = true)
 |    |    |    |-- date: string (nullable = true)
 |    |    |-- email: string (nullable = true)
 |    |    |-- gender: string (nullable = true)
 |    |    |-- id: struct (nullable = true)
 |    |    |    |-- name: string (nullable = true)
 |    |    |    |-- value: string (nullable = true)
 |    |    |-- location: struct (nullable = true)
 |    |    |    |-- city: string (nullable = true)
 |    |    |    |-- coordinates: struct (nullable = true)
 |    |    |    |    |-- latitude: string (nullable = true)
 |    |    |    |    |-- longitude: string (nullable = true)
 |    |    |    |-- country: string (nullable = true)
 |    |    |    |-- postcode: long (nullable = true)
 |    |    |    |-- state: string (nullable = true)
 |    |    |    |-- street: struct (nullable = true)
 |    |    |    |    |-- name: string (nullable = true)
 |    |    |    |    |-- number: long (nullable = true)
 |    |    |    |-- timezone: struct (nullable = true)
 |    |    |    |    |-- description: string (nullable = true)
 |    |    |    |    |-- offset: string (nullable = true)
 |    |    |-- login: struct (nullable = true)
 |    |    |    |-- md5: string (nullable = true)
 |    |    |    |-- password: string (nullable = true)
 |    |    |    |-- salt: string (nullable = true)
 |    |    |    |-- sha1: string (nullable = true)
 |    |    |    |-- sha256: string (nullable = true)
 |    |    |    |-- username: string (nullable = true)
 |    |    |    |-- uuid: string (nullable = true)
 |    |    |-- name: struct (nullable = true)
 
 =========================================================================================================
